{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "The next code block sets parameters that are used throughout the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# one of 'positive' or 'negative'\n",
    "polarity = 'positive'\n",
    "\n",
    "# one of 'ISTDsEtc' or 'FinalEMA-HILIC'\n",
    "output_type = 'FinalEMA-HILIC'\n",
    "\n",
    "# an integer, increment if you need to redo your analysis\n",
    "# will be appended to your username to create analysis_id\n",
    "analysis_number = 0\n",
    "\n",
    "# experiment ID that must match the parent folder containing the LCMS output files\n",
    "experiment = '20201116_JGI-AK_LH_506489_SoilWarm_final_QE-HF_HILICZ_USHXG01530'\n",
    "\n",
    "# Exclude files with names containing any of the substrings in this list. Eg., ['peas', 'beans']\n",
    "exclude_files = []\n",
    "\n",
    "# Exclude groups with names containing any of the substrings in this list.\n",
    "# 'POS' or 'NEG' will be auto-appended later, so you shouldn't use them here.\n",
    "exclude_groups = ['QC','InjBl']\n",
    "\n",
    "# include MSMS fragment ions in the output documents?\n",
    "export_msms_fragment_ions = False\n",
    "\n",
    "# list of substrings that will group together when creating groups\n",
    "# this provides additional grouping beyond the default grouping on field #12\n",
    "groups_controlled_vocab = ['QC','InjBl','ISTD']\n",
    "\n",
    "# list of tuples contain string with color name and substring pattern.\n",
    "# Lines in the EIC plot will be colored by the first substring pattern\n",
    "# that has a match within the name of the hdf5_file. The order they are\n",
    "# listed in your list is the order they are displayed in the overlays\n",
    "# (first is front, last is back). Named colors available in matplotlib\n",
    "# are here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "# or use hexadecimal values '#000000'. Lines default to black.\n",
    "rt_adjuster_color_list = [('red','ExCtrl'),                                                     \n",
    "                          ('green','TxCtrl'),\n",
    "                          ('blue','InjBl')]\n",
    "\n",
    "\n",
    "\n",
    "# The rest of this block contains project independent parameters\n",
    "\n",
    "# Full path to the directory where you have cloned the metatlas git repo.\n",
    "# If you ran the 'git clone ...' command in your home directory on Cori, \n",
    "# then you'll want '/global/homes/FIRST-INITIAL-OF-USERNAME/USERNAME/metatlas'\n",
    "# where the uppercase letters are replaced based on your NERSC username.\n",
    "metatlas_repo_path = '/global/homes/FIRST-INITIAL-OF-USERNAME/USERNAME/metatlas'\n",
    "\n",
    "\n",
    "# Full path to the directory where you want this notebook to store data.\n",
    "# A subdirectory will be auto created within this directory for each project.\n",
    "# You can place this anywhere on cori's filesystem, but placing it within your\n",
    "# global home directory is recommended so that you do not need to worry about\n",
    "# your data being purged. Each project will take on the order of 100 MB.\n",
    "project_directory = '/global/homes/FIRST-INITIAL-OF-USERNAME/USERNAME/metabolomics_projects'\n",
    "\n",
    "# maximum number of CPUs to use\n",
    "# when running on jupyter.nersc.gov, you are not allowed to set this above 4\n",
    "max_cpus = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%env HDF5_USE_FILE_LOCKING=FALSE\n",
    "\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(0, metatlas_repo_path)\n",
    "try:\n",
    "    import dataset\n",
    "except ModuleNotFoundError:\n",
    "    print('Could not find dataset module. Please check that the kernel is set to \"metatlas py3\".')\n",
    "    raise ValueError('Invalid kernel setting in Jupyter Notebook.')\n",
    "try:\n",
    "    from metatlas.tools import fastanalysis as fa\n",
    "except ModuleNotFoundError:\n",
    "    print('Could not find metatlas module. In the Parameters block, please check the value of metatlas_repo_path.')\n",
    "    raise ValueError('Invalid metatlas_repo_path parameter in Jupyter Notebook.')\n",
    "from metatlas.plots import dill2plots as dp\n",
    "from metatlas.io import metatlas_get_data_helper_fun as ma_data\n",
    "from metatlas.plots import chromplotplus as cpp\n",
    "from metatlas.datastructures import metatlas_objects as metob\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pathlib import Path\n",
    "import getpass\n",
    "import time\n",
    "import pickle\n",
    "import dill\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import operator\n",
    "from importlib import reload\n",
    "\n",
    "if polarity not in ['positive', 'negative']:\n",
    "    raise ValueError('Parameter polarity is not one of \"positive\" or \"negative\".')\n",
    "\n",
    "if output_type == 'ISTDsEtc':\n",
    "    pass\n",
    "elif output_type == 'FinalEMA-HILIC':\n",
    "    num_data_points_passing = 5\n",
    "    peak_height_passing = 4e5\n",
    "else:\n",
    "    raise ValueError('Parameter output_type is not one of \"ISTDsEtc\" or \"FinalEMA-HILIC\".')\n",
    "\n",
    "if len(experiment.split('_')) != 9:\n",
    "    raise ValueError('Parameter experiment does contain 9 fields when split on \"_\".')\n",
    "    \n",
    "username = getpass.getuser()\n",
    "analysis_id = f\"{username}{analysis_number}\"\n",
    "output_dir = os.path.join(project_directory, experiment, analysis_id, output_type)\n",
    "short_experiment_analysis_id = experiment.split('_')[0]+'_'+experiment.split('_')[3]+'_'+analysis_id\n",
    "\n",
    "if not os.path.exists(project_directory):\n",
    "    os.makedirs(project_directory)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "from  IPython.core.display  import  display, HTML\n",
    "# set notebook to have minimal side margins\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Groups (named variables that hold your replicates of each sample)\n",
    "\n",
    "### You must assign your raw files into experimental groups for analysis.  These are used for downstream statistics and for selection of specific groups for filtering to subsets of files for analysis (Ex. just pos or just neg).\n",
    "\n",
    "The groups are created from common file headers and the unique group names. The convention our lab group uses for filenames is as follows: \n",
    "***\n",
    "DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ_SAMPLENUMBER_ SAMPLEGROUP_REP_OPTIONAL_SEQ \n",
    "\n",
    "Ex.:20180105_SK_AD_ENIGMA_PseudoInt_R2ADec2017_QE119_50454_123456_POS_MSMS_001_Psyringae-R2A-30C-20hr_Rep01_NA_Seq001.raw\n",
    "***\n",
    "The common header consists of the fields 0-10: DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ \n",
    "\n",
    "The sample group name is commonly field # 12 (between underscore 11 and 12) -0 indexed-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find your files\n",
    "1. On the first line of the block below, set the 'experiment' and 'name' variables to find your files.  These fields require wildcards for partial string searches\n",
    "2. 'Experiment' is the folder name within global/project/projectdirs/metatlas/raw_data, that will be emailed to you when the files are uploaded to NERSC.  You can also look in the raw_data directory for the NERSC user who uploaded your files; your experiment folder should be in there.\n",
    "3. 'name' is string that will match a subset of your files within that folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dp.get_metatlas_files(experiment = experiment,name = '%',most_recent = True)\n",
    "df = metob.to_dataframe(files)\n",
    "print(f\"Number of LCMS output files matching '{experiment}' is: {len(files)}.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION A: Automated Group Maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will attempt to create groups in an automated fashion (rather than filling out a spreadsheet with a list of files and group names).  If your files are all in one folder at nersc, you can use this options.  If not, use option B below.\n",
    "\n",
    "A long group name consisting of the common header + either controlled vocab value or field #12 along with a short group name (just controlled vocab or field #12) will be stored in a local variable.  The short group names can be used on plots.\n",
    "\n",
    "\n",
    "1. STEP 1: View the groups\n",
    "    1. Pick an experiment folder to look for files in on the metob.retrieve function\n",
    "    2. Enter controlled vocabulary for control files to put select files into groups when control string may be in a different field (not #12) or as a randomly placed substring within a field (ex. if 'InjBl' is included in your controlled vocab list, files like _InjBl-MeOH_ and _StartInjBl_ will group together)\n",
    "    3. If your group name is not between _ 11 and 12 you can adjust those values in the split commands below.  All other (non-controlledvocab) groups will be created from that field.\n",
    "2. STEP 2: Create the groups variable after checking the output from STEP 1\n",
    "3. STEP 3: <br />\n",
    "    Option A: If everything looks fine the group names and short names, Store groups once you know you have files in correct groups by running and checking the output of STEPS 1 and 2.<br />\n",
    "    Option B (optional): If you would like to edit the groups, uncomment the options B-I and B-II. Run Option B-I to export a prefilled tab infosheet. Edit the file and then run Option B-II to import the new groups and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: View the groups\n",
    "files = dp.get_metatlas_files(experiment = experiment,name = '%',most_recent = True)\n",
    "file_dict = {}\n",
    "groups_dict = {}\n",
    "for f in files:\n",
    "    if not any(map(f.name.__contains__, exclude_files)):\n",
    "        k = f.name.split('.')[0]\n",
    "        #     get index if any controlled vocab in filename\n",
    "        indices = [i for i, s in enumerate(groups_controlled_vocab) if s.lower() in k.lower()]\n",
    "        prefix = '_'.join(k.split('_')[:11])\n",
    "        if len(indices)>0:\n",
    "            short_name = groups_controlled_vocab[indices[0]].lstrip('_')\n",
    "            group_name = '%s_%s_%s'%(prefix,analysis_id,short_name)\n",
    "            short_name = k.split('_')[9]+'_'+short_name # Prepending POL to short_name\n",
    "        else:\n",
    "            short_name = k.split('_')[12]\n",
    "            group_name = '%s_%s_%s'%(prefix,analysis_id,short_name)\n",
    "            short_name = k.split('_')[9]+'_'+k.split('_')[12]  # Prepending POL to short_name\n",
    "        file_dict[k] = {'file':f,'group':group_name,'short_name':short_name}\n",
    "        groups_dict[group_name] = {'items':[],'name':group_name,'short_name':short_name}\n",
    "df = pd.DataFrame(file_dict).T\n",
    "df.index.name = 'filename'\n",
    "df.reset_index(inplace=True)#['group'].unique()\n",
    "df.drop(columns=['file'],inplace=True)\n",
    "for ug in groups_dict.keys():\n",
    "    for file_key,file_value in file_dict.items():\n",
    "        if file_value['group'] == ug:\n",
    "            groups_dict[ug]['items'].append(file_value['file'])\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: create the groups variable, if the above looks OK\n",
    "\n",
    "groups = []\n",
    "for group_key,group_values in groups_dict.items():\n",
    "    g = metob.Group(name=group_key,items=group_values['items'],short_name=group_values['short_name'])\n",
    "    groups.append(g)        \n",
    "    for item in g.items:\n",
    "        print(g.name,g.short_name,item.name)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 Option A: store the groups variable content in the DB (currently only the long group name is stored)\n",
    "metob.store(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 3 Option B-I: OPTIONAL: Export groups to csv file for editing (filename, short_name, group, description)\n",
    "#dp.make_prefilled_fileinfo_sheet(groups,os.path.join(output_dir,'prefilled_fileinfo.tab'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 3 Option B-II: Import groups from csv file after editing the prefilled_fileinfo.tab\n",
    "#groups = dp.make_groups_from_fileinfo_sheet(os.path.join(output_dir,'prefilled_fileinfo.tab'), filetype='tab', store=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION B: Register LCMS Runs into categorical groups from a file.\n",
    "\n",
    "Typically, you will make one fileinfo sheet with all of your files (pos and neg) for this experiment.  At a minimum, group names MUST contain the first 11 underscore delimited fields (DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ) and the 'SAMPLEGROUP' field.\n",
    "\n",
    "Files can be from multiple folders at nersc.\n",
    "\n",
    "1. STEP 1: select files\n",
    "    1. Edit the experiment and name fields to find the files you want.\n",
    "2. STEP 2: create and save a .tab file to your project directory.\n",
    "    1. After running the block, find the .tab file in your project directory.\n",
    "    2. Open in excel or other spreadsheet editor.\n",
    "    3. Fill out the group names as per above in the editor.\n",
    "    4. Save the file as filled_fileinfo.txt\n",
    "3. STEP 3: Create groups from spreadsheet\n",
    "    1. Transfer the .txt. file back to your project directory \n",
    "    2. Run the make groups block using store=False\n",
    "4. STEP 4: CHECK groups\n",
    "    1. Run the next block 'metob.to_dataframe(g) and check that the information looks correct\n",
    "5. If it is correct, rerun the STEP 2 make groups block, using store=True.  If not, fix your file in excel and redo Steps 2&3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP 1: Select files\n",
    "# files = dp.get_metatlas_files(experiment =experiment,name = '%',most_recent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP 2: Save spreadsheet file\n",
    "# dp.make_empty_fileinfo_sheet('%s%s' % (output_dir,'empty_fileinfo.tab'),files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #STEP 3: create groups from file\n",
    "# g = dp.make_groups_from_fileinfo_sheet('%s%s' % (output_dir,'filled_fileinfo.txt'),\n",
    "#                                        filetype='tab',\n",
    "#                                        store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # STEP 4: check groups\n",
    "# metob.to_dataframe(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data frame of short filenames and samplenames\n",
    "Uncomment the below 2 blocks to make short file names and smaple names.<br>\n",
    "This creates a dataframe and a csv file which can be edited, exported and imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make short_filename and short_samplename \n",
    "files = metob.retrieve('lcmsruns',experiment=experiment,username='*')\n",
    "short_filename_delim_ids = [0,2,4,5,7,9,14]\n",
    "short_samplename_delim_ids = [9,12,13,14]\n",
    "short_names_df = pd.DataFrame(columns=['sample_treatment','short_filename','short_samplename'])\n",
    "ctr = 0\n",
    "for f in files:\n",
    "    short_filename = []\n",
    "    short_samplename = []\n",
    "    tokens = f.name.split('.')[0].split('_')\n",
    "    for id in short_filename_delim_ids:\n",
    "        short_filename.append(str(tokens[id]))\n",
    "    for id in short_samplename_delim_ids:\n",
    "        short_samplename.append(str(tokens[id]))\n",
    "    short_filename = \"_\".join(short_filename)\n",
    "    short_samplename = \"_\".join(short_samplename)\n",
    "    short_names_df.loc[ctr, 'full_filename'] = f.name.split('.')[0]\n",
    "    short_names_df.loc[ctr, 'sample_treatment'] = str(tokens[12]) # delim 12\n",
    "    short_names_df.loc[ctr, 'short_filename'] = short_filename\n",
    "    short_names_df.loc[ctr, 'short_samplename'] = short_samplename\n",
    "    short_names_df.loc[ctr, 'last_modified'] = pd.to_datetime(f.last_modified,unit='s')\n",
    "    ctr +=1\n",
    "short_names_df.sort_values(by='last_modified', inplace=True)\n",
    "short_names_df.drop(columns=['last_modified'], inplace=True)\n",
    "short_names_df.drop_duplicates(subset=['full_filename'], keep='last', inplace=True)\n",
    "short_names_df.set_index('full_filename', inplace=True)\n",
    "short_names_df.to_csv(os.path.join(output_dir, 'short_names.csv'), sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional import edited short_names.csv \n",
    "# short_names_df = pd.read_csv(os.path.join(output_dir, 'short_names.csv'), sep=',', index_col='full_filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Select groups of files to operate on\n",
    "\n",
    "Here, you will assign your database groups to a local variable which will be used downstream in the notebook for analyzing your data with an atlas.\n",
    "\n",
    "1. in block below, fill out the fields for name, include_list and exclude_list using text strings from the group names you created in the previous step.  The include/exlcude lists do not need wildcards.  Name is a string unique to all of your groups (ex. fields 0-11 of your filenames)\n",
    "\n",
    "### Typically, you will run one polarity at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_groups.append('NEG' if polarity=='positive' else 'POS')\n",
    "groups = dp.select_groups_for_analysis(name = experiment+'%',\n",
    "                                       most_recent = True,\n",
    "                                       remove_empty = True,\n",
    "                                       include_list = [], exclude_list = exclude_groups)\n",
    "print(\"sorted groups\")\n",
    "groups = sorted(groups, key=operator.attrgetter('name'))\n",
    "for i,a in enumerate(groups):\n",
    "    print(i, a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view metadata about your groups, run the block below\n",
    "metob.to_dataframe(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create new Atlas entries in the Metatlas database from a csv file\n",
    "\n",
    "## QC, IS, and EMA template atlases are available on the google drive.\n",
    "\n",
    "1. Create your atlas as a csv file, check that it looks correct (has all the correct headers and no blank values in rows; all columns are the correct data type\n",
    "2. Save it with the type of atlas (EMA, QC or IS), your initials, the experiment name, the polarity, and the version or timestamp\n",
    "3. Upload it to your nersc project directory (the one you named above).  (If it doesn't work, double check your file permissions are set to at least rw-rw----).\n",
    "4. Run blocks below to create the DB entries for negative and positive mode atlases\n",
    "5. WARNING: Don't run this block over and over again - it will create multiple new DB entries with the same atlas name\n",
    "\n",
    "Required Atlas file headers:\n",
    "\n",
    "inchi_key,label,rt_min,rt_max,rt_peak,mz,mz_tolerance,adduct,polarity,identification_notes\n",
    "\n",
    "values in rows must be completed for all fields except inchi_key (leaving this blank will not allow you to perform MSMS matching below), and identification notes\n",
    "\n",
    "INFO: store=True will register your atlas in the database.  If you are not sure if your atlas structure is correct, set store=False for the first time your run the block to check if you get an error.  If there is no error, then rerun it with store=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATLAS UPLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atlas_file_name = '' # <- enter the exact name of your csv file including the full path and .csv file extension\n",
    "\n",
    "if atlas_file_name != '':\n",
    "    names = dp.make_atlas_from_spreadsheet(atlas_file_name,\n",
    "                                           Path(atlas_file_name).stem,\n",
    "                                           filetype='csv',\n",
    "                                           sheetname='',\n",
    "                                           polarity = polarity,\n",
    "                                           store=True,\n",
    "                                           mz_tolerance = 12\n",
    "                                          )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Select Atlas to use\n",
    "\n",
    "1. The first block will retrieve a list of atlases matching the 'name' string that you enter.  Also, you must enter your username.\n",
    "2. The next block will select one from the list, using the index number.  Make sure to enter the index number for the atlas you want to use for your analysis by setting in this line: my_atlas = atlases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_query = f\"%_{polarity[:3].upper()}_{short_experiment_analysis_id}%\"\n",
    "atlases = metob.retrieve('atlases', name=f\"%_{polarity[:3].upper()}_{short_experiment_analysis_id}%\", username=username)\n",
    "for i,a in enumerate(atlases):\n",
    "    print(i,a.name,pd.to_datetime(a.last_modified,unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_atlas = atlases[-1]\n",
    "atlas_df = ma_data.make_atlas_df(my_atlas)\n",
    "atlas_df['label'] = [cid.name for cid in my_atlas.compound_identifications]\n",
    "print(my_atlas.name)\n",
    "metob.to_dataframe([my_atlas])\n",
    "# the first line of the output will show the dimensions of the atlas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Get EICs and MSMS for all files in your groups, using all compounds in your atlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This block builds the metatlas_dataset variable.  This holds your EIC data (mz, rt, intensity values within your mz and rt ranges).\n",
    "\n",
    "The EIC data contains mz, intensity and RT values across your RT range.  There are two parameters that you will need to edit: extra_time and extra_mz.  Extra time will collect mz, intensity and RT values from outside of your atlas defined min and max rt values.  For example if your rt_min = 1.0, and rt_max = 2.0 and you set extra_time to 0.3, then your new rt range will be 0.7 to 2.3.  This is helpful for checking if you have nearby peaks at the same m/z.  Extra_mz should only be used for troubleshooting.  You should keep this at 0 unless you believe you have poor mass accuracy during your run.  Other ways to address this issue is by changing the mz_tolerance values in your atlas.  Before changing this value, you should check in with a metatlas experienced lab member to discuss when/how to use this value.\n",
    "\n",
    "1. Change the value in \"extra_time = 0.0\" to something like 0.5 to 1.0 for the first EMA runthrough on your files.  This will take longer but collect msms outside your retention windows which allows you to check the msms of nearby peaks before adjusting your rt bounds around the correct peak.\n",
    "2. extra_mz should almost always be set to 0.0   If you need to troubleshoot a low mz compound you could potentially use this value to run it back through with a larger mz error window than what was specified in your atlas (ppm tolerance).\n",
    "\n",
    ">On Your final runthrough, set extra_time to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for my_group in groups:\n",
    "    for my_file in my_group.items:\n",
    "        extra_time = 0.75          # NOTE: 0.75 for the first run, 0.5 for final \n",
    "        extra_mz = 0.00\n",
    "        all_files.append((my_file,my_group,atlas_df,my_atlas,extra_time,extra_mz))\n",
    "pool = mp.Pool(processes=min(max_cpus, len(all_files)))\n",
    "t0 = time.time()\n",
    "metatlas_dataset = pool.map(ma_data.get_data_for_atlas_df_and_file, all_files)\n",
    "pool.close()\n",
    "pool.terminate()\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data sources tables (atlas_metadata.tab, groups_metadata.tab, groups.tab and [atlasname]_originalatlas.tab within data_sources subfolder)\n",
    "ma_data.make_data_sources_tables(groups, my_atlas, output_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6b Optional: Filter atlas for compounds with no or low signals\n",
    "\n",
    "Uncomment the below 3 blocks to filter the atlas.\n",
    "Please ensure that correct polarity is used for the atlases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp = reload(dp)\n",
    "# num_data_points_passing = 5\n",
    "# peak_height_passing = 4e5\n",
    "# atlas_df_passing = dp.filter_atlas(atlas_df=atlas_df, input_dataset=metatlas_dataset, num_data_points_passing = num_data_points_passing, peak_height_passing = peak_height_passing)\n",
    "# print(\"# Compounds in Atlas: \"+str(len(atlas_df)))\n",
    "# print(\"# Compounds passing filter: \"+str(len(atlas_df_passing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new atlas and store in database\n",
    "This block creates a filtered atlas with a new name !!\n",
    "Automatically selects this atlas for processing. \n",
    "Make sure to use this atlas for downstream analyses. (NOTE: If you restart kernel or come back to the analysis, you need to reselect this newly created filtered atlas for processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlas_passing = my_atlas.name+'_filteredby-datapnts'+str(num_data_points_passing)+'-pkht'+str(peak_height_passing)\n",
    "# myAtlas_passing = dp.make_atlas_from_spreadsheet(atlas_df_passing,\n",
    "#                           atlas_passing,\n",
    "#                           filetype='dataframe',\n",
    "#                           sheetname='',\n",
    "#                           polarity = polarity,\n",
    "#                           store=True,\n",
    "#                           mz_tolerance = 12)\n",
    "\n",
    "# atlases = dp.get_metatlas_atlas(name=atlas_passing,do_print = True, most_recent=True)\n",
    "\n",
    "# myAtlas = atlases[-1]\n",
    "# atlas_df = ma_data.make_atlas_df(myAtlas)\n",
    "# atlas_df['label'] = [cid.name for cid in myAtlas.compound_identifications]\n",
    "# print(myAtlas.name)\n",
    "# print(myAtlas.username)\n",
    "# metob.to_dataframe([myAtlas])# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files = []\n",
    "# for my_group in groups:\n",
    "#     for my_file in my_group.items:\n",
    "#         all_files.append((my_file,my_group,atlas_df,myAtlas))\n",
    "        \n",
    "# pool = mp.Pool(processes=min(max_cpus, len(all_files)))\n",
    "# t0 = time.time()\n",
    "# metatlas_dataset = pool.map(ma_data.get_data_for_atlas_df_and_file, all_files)\n",
    "# pool.close()\n",
    "# pool.terminate()\n",
    "# #If you're code crashes here, make sure to terminate any processes left open.\n",
    "#(print time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One of the two blocks below builds the hits variable.  This holds your MSMS spectra (from within your mz, and rt ranges, and within the extra time indicated above).\n",
    "\n",
    "There are two options for generating the hits variable:\n",
    "1. block A: use when your files have msms. It create the hits variable and also saves a binary (pickled) serialized hits file to the output directory.\n",
    "2. block B: only run if your files were collected in MS1 mode\n",
    "3. If you have already run block A and then the kernel dies, you can skip block A and directly unplickle the binary hits file from the output directory. Skip block A, uncomment the Optional block and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BLOCK A\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "t0 = time.time()\n",
    "\n",
    "hits=dp.get_msms_hits(metatlas_dataset,extra_time=True,keep_nonmatches=True, frag_mz_tolerance=0.01, ref_loc='/global/project/projectdirs/metatlas/projects/spectral_libraries/msms_refs_v3.tab')\n",
    "pickle.dump(hits, open(os.path.join(output_dir,polarity[:3].upper()+'_hits.pkl'), \"wb\"))\n",
    "\n",
    "print(time.time() - t0)\n",
    "print('%s%s' % (len(hits),' <- total number of MSMS spectra found in your files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BLOCK B (uncomment lines below to run this.  Only use when all data files are MS1)\n",
    "#hits=pd.DataFrame([], columns=['database','id','file_name','msms_scan', u'score', u'num_matches', u'msv_query_aligned', u'msv_ref_aligned', u'name', u'adduct', u'inchi_key', u'precursor_mz', u'measured_precursor_mz'])\n",
    "#hits.set_index(['database','id','file_name','msms_scan'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: If you already have a pickled hits file and do not need to run get_msms_hits again, uncomment this block\n",
    "# hits = pickle.load(open(os.path.join(output_dir,polarity[:3].upper()+'_hits.pkl'), \"rb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adjust Retention Times. \n",
    "\n",
    "This block creates an interactive plot.  The top panel displays MSMS from within the two green RT bounds selected below (rt min and max, initially set in atlas).  When the database holds reference spectra, mirror plots are generated with the reference spectra inverted below the sample spectra.  The lower panel displays the EICs overlayed for all of the files in your selected groups.  You can highlight your groups different colors.  It is recommended that you do this, at least, for your extraction blank (or if not available, use a solvent injection blank).  This plot also displays radio buttons that can be interactively selected; the values will be exported in your final identifications table and in your atlas export.  Use these to mark peak/MSMS quality.\n",
    "\n",
    "How to use:\n",
    "1. STEP 1: Set peak flag radio buttons\n",
    "    1. OPTION A (custom flags): fill out the peak flags list (list of strings) \n",
    "        peak_flag_list = ('A','B') some recommendations are below.  \n",
    "    2. OPTION B (default flags): comment out the custom peak_flag_list line.  Uncomment the default peak_flags = \"\". \n",
    "        Flags default to: keep, remove, unresolvable isomers, check.\n",
    "2. STEP 2: Set EIC colors\n",
    "    1. Option A (custom EIC colors): fill out the colorlist in the format of below\n",
    "   > ***\n",
    "   > colorlist = [['color1nameorhexadec','partialgroupstring1'],\n",
    "   >         ['color2nameorhexadec','partialgroupstring2']]\n",
    "   > ***\n",
    "    <ul><li>You can add more comma delimited colors/groups as needed.</li>\n",
    "    <li>These are partial strings that match to you file names (not your group names).</li>\n",
    "    <li>The order they are listed in your list is the order they are displayed in the overlays (first is front, last is back)</li>\n",
    "    <li>Named colors available in matplotlib are here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "          or use hexadecimal values '#000000'</li></ul>\n",
    "    B. Option B (default EIC colors): comment out the custom colorlist lines and uncomment the default colorlist = \"\". \n",
    "        Colors all default to black.\n",
    "3. User the right/left buttons on your keyboard to cycle through compounds in your atlas.\n",
    "4. Use the up/down buttons on your keyboard to cycle through MSMS spectra within the RT bounds of the lower plot.\n",
    "5. Use the horizontal rt min and rt max bars below the plots to adjust the rt bounds around your peak.  If there are multiple peaks, select one at a time and then click up/down to update the msms available in that new RT range.  If necessary evaluate your data in an external program such as mzmine to make sure you are selecting the correct peak.\n",
    "\n",
    "TIPS: use compound_idx = 0 in step 3 to change to a different compound in your atlas using the index number.  If your plot does not fit in your browser window, adjust height and width values.  Use alpha to change the transparency of the lines this is a value 0 (transparent) to 1 (opaque).\n",
    "\n",
    "DO NOT change your RT theoretical peak (the purple line).  It is locked from editing (unless you change a hidden parameter) and only to be changed in special cases.  The measured retention times of your peaks will be calculated and exported in your output files.  These will be compared with the RT theoreticals and used in your evidence of identification table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "a = dp.adjust_rt_for_selected_compound(metatlas_dataset, msms_hits=hits,\n",
    "                                       color_me=rt_adjuster_color_list,\n",
    "                                       compound_idx=0, alpha=0.5, width=18, height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create filtered atlas excluding compounds marked removed\n",
    "\n",
    "Re-run the following before filtering atlas\n",
    "1. Get Groups (include InjBl)\n",
    "2. Get Atlas\n",
    "3. Get Data\n",
    "4. Get MSMS Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(atlas_all, atlas_kept, atlas_removed) = dp.filter_by_remove(atlas_df, metatlas_dataset)\n",
    "print(\"# Compounds Total: \"+str(len(atlas_all)))\n",
    "print(\"# Compounds Kept: \"+str(len(atlas_kept)))\n",
    "print(\"# Compounds Removed: \"+str(len(atlas_removed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlasfilename=my_atlas.name+'_kept'  # <- enter the name of the atlas to be stored\n",
    "\n",
    "names = dp.make_atlas_from_spreadsheet(atlas_kept, \n",
    "                                       atlasfilename,  # <- DO NOT EDIT THIS LINE\n",
    "                                       filetype='dataframe',\n",
    "                                       sheetname='',\n",
    "                                       polarity = polarity,\n",
    "                                       store=True,\n",
    "                                       mz_tolerance = 12\n",
    "                                      )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the following before filtering atlas\n",
    "1. Restart kernel\n",
    "2. Get Groups\n",
    "3. Get Atlas (look for the *_kept atlas)\n",
    "4. Get Data\n",
    "5. Get MSMS Hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export results files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Atlas to a Spreadsheet\n",
    "\n",
    "The peak flags that you set and selected from the rt adjuster radio buttons will be saved in a column called id_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_atlas_filename = os.path.join(output_dir, f\"{polarity[:3].upper()}_{my_atlas.name}export\")\n",
    "atlas_identifications = dp.export_atlas_to_spreadsheet(my_atlas, export_atlas_filename)\n",
    "export_atlas_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS match scores, stats sheets, and final identification table\n",
    "\n",
    "This block creates a number of files:\n",
    "\n",
    "1. compound_scores.csv\n",
    "2. stats_table.tab\n",
    "3. filtered and unfiltered peak heights, areas, msms scores, mz centroid, mz ppm error, num of fragment matches, rt delta, rt peak\n",
    "4. final identification sheet that is formatted for use as a supplemental table for manuscript submission.  You will need to manually complete some columns.  Please discuss with Ben, Katherine, Daniel or Suzie before using for the first time.\n",
    "\n",
    "THe kwargs below will set the filtering points for the parameters indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'min_intensity': 1e4,   # strict = 1e5, loose = 1e3\n",
    "          'rt_tolerance': .5,    #>= shift of median RT across all files for given compound to reference\n",
    "          'mz_tolerance': 20,      # strict = 5, loose = 25; >= ppm of median mz across all files for given compound relative to reference\n",
    "          'min_msms_score': .6, 'allow_no_msms': True,     # strict = 0.6, loose = 0.3 <= highest compound dot-product score across all files for given compound relative to reference\n",
    "          'min_num_frag_matches': 1, 'min_relative_frag_intensity': .001}   # strict = 3 and 0.1, loose = 1, 0.01 number of matching mzs when calculating max_msms_score and ratio of second highest to first highest intensity of matching sample mzs\n",
    "scores_df = fa.make_scores_df(metatlas_dataset,hits)\n",
    "scores_df['passing'] = fa.test_scores_df(scores_df, **kwargs)\n",
    "\n",
    "pass_atlas_df, fail_atlas_df, pass_dataset, fail_dataset = fa.filter_atlas_and_dataset(scores_df, atlas_df, metatlas_dataset, column='passing')\n",
    "\n",
    "fa.make_stats_table(input_dataset = metatlas_dataset, msms_hits = hits, output_loc = output_dir,min_peak_height=1e5,use_labels=True,min_msms_score=0.01,min_num_frag_matches=1,include_lcmsruns = [],exclude_lcmsruns = ['QC'], polarity=polarity[:3].upper())\n",
    "scores_df.to_csv(os.path.join(output_dir,'stats_tables',polarity[:3].upper()+'_compound_scores.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export EIC chromatograms as individual pdfs for each compound\n",
    "\n",
    "1.  There are three options for formatting your EIC output using the \"group =\" line below:\n",
    "    1. 'page' will print each sample group on a new page of a pdf file\n",
    "    2. 'index' will label each group with a letter\n",
    "    3. None will print all of the groups on one page with very small subplot labels\n",
    "2. The Y axis scale can be shared across all files using share_y = True or set to the max within each file using share_y = False\n",
    "3. To use short names for plots, short_names_df should be provided as input. Additionally the header column to be used for short names should be provided as follows (short_names_df=short_names_df, short_names_header='short_samplename'). Header options are sample_treatment, short_filename, short_samplename. These are optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = 'index' # 'page' or 'index' or None\n",
    "save = True\n",
    "share_y = True\n",
    "\n",
    "dp.make_chromatograms(input_dataset=metatlas_dataset, include_lcmsruns = [],exclude_lcmsruns = ['InjBl','QC','Blank','blank'], group=group, share_y=share_y, save=save, output_loc=output_dir, short_names_df=short_names_df, short_names_header='short_samplename', polarity=polarity[:3].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS mirror plots as individual pdfs for each compound\n",
    "\n",
    "1. use_labels = True will use the compound names you provided in your atlas, if you set it to false, the compounds will be named with the first synonym available from pubchem which could be a common name, iupac name, cas number, vendor part number, etc. \n",
    "2.  The include and exclude lists will match partial strings in filenames, do not use wildcards.\n",
    "3. If short_names_df is provided as input, short_samplename is used for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dp.make_identification_figure_v2(input_dataset = metatlas_dataset, msms_hits=hits, use_labels=True, include_lcmsruns = [],exclude_lcmsruns = ['InjBl','QC','Blank','blank'], output_loc=output_dir,  short_names_df=short_names_df, polarity=polarity[:3].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sheets\n",
    "1. To include short names in the output, short_names_df should be provided as input to make_output_dataframe. \n",
    "2. ylabel is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_height = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='peak_height', output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)\n",
    "peak_area = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='peak_area', output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)\n",
    "mz_peak = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='mz_peak', output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)\n",
    "rt_peak = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [],fieldname='rt_peak', output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)\n",
    "mz_centroid = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='mz_centroid', output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)\n",
    "rt_centroid = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='rt_centroid', output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.make_boxplot_plots(rt_peak, output_loc=os.path.join(output_dir, polarity[:3].upper()+'_boxplot_rt_peak'), ylabel=\"RT Peak\")\n",
    "dp.make_boxplot_plots(peak_height, output_loc=os.path.join(output_dir, polarity[:3].upper()+'_boxplot_peak_height'), ylabel=\"Peak Height\")\n",
    "dp.make_boxplot_plots(mz_centroid, output_loc=os.path.join(output_dir, polarity[:3].upper()+'_boxplot_mz_centroid'), ylabel=\"MZ Centroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS fragment Ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_msms_fragment_ions:\n",
    "    intensity_fraction = 0.01\n",
    "    min_mz = 450.0 #minimum m/z to export in msms\n",
    "    max_mz = -40.0 # distance from precurosor to export (0.5 is a good number. crazy people use negative numbers)\n",
    "    scale_intensity = True\n",
    "    data = []\n",
    "    for compound_index in range(len(metatlas_dataset[0])):\n",
    "        max_intensity = 0\n",
    "        d = {}\n",
    "        for file_index in range(len(metatlas_dataset)):\n",
    "            try:\n",
    "                pk_idx = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['precursor_intensity'].argmax()\n",
    "                pk = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['precursor_intensity'][pk_idx]\n",
    "                precursor_mz = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['precursor_MZ'][pk_idx]\n",
    "                rt = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['rt'][pk_idx]\n",
    "                if (pk>max_intensity) & (rt>metatlas_dataset[file_index][compound_index]['identification'].rt_references[-1].rt_min) & (rt<metatlas_dataset[file_index][compound_index]['identification'].rt_references[-1].rt_max):\n",
    "                    good_index = file_index\n",
    "                    max_intensity = pk\n",
    "                    final_mz = precursor_mz #save this for filtering below\n",
    "            except:\n",
    "                pass\n",
    "    #     print(compound_index,good_index,max_intensity)\n",
    "        if max_intensity>0:\n",
    "            msms = metatlas_dataset[good_index][compound_index]['data']['msms']['data']\n",
    "            idx = np.argwhere(msms['precursor_intensity']==max_intensity).flatten()\n",
    "            mz = msms['mz'][idx]\n",
    "            intensity = msms['i'][idx]\n",
    "            max_msms_intensity = intensity.max()\n",
    "            cutoff = intensity_fraction * max_msms_intensity\n",
    "            conditions = (intensity>cutoff) & (mz>min_mz) & (mz<(final_mz+max_mz))\n",
    "            if sum(conditions)>0:\n",
    "                keep_idx = np.argwhere(conditions).flatten()\n",
    "                mz = str(['%.2f'%x for x in list(mz[keep_idx])]).replace('\\'','')\n",
    "                if scale_intensity==True:\n",
    "                    intensity = intensity / intensity.max()\n",
    "                    intensity = intensity * 1e5\n",
    "                    intensity = intensity.astype(int)\n",
    "                intensity = str(['%d'%x for x in list(intensity[keep_idx])]).replace('\\'','')\n",
    "                spectra = str([mz,intensity]).replace('\\'','')\n",
    "            else:\n",
    "                mz = None\n",
    "                intensity = None\n",
    "                spectra = None\n",
    "        else:\n",
    "            mz = None\n",
    "            intensity = None\n",
    "            spectra = None\n",
    "        data.append({'name':metatlas_dataset[file_index][compound_index]['identification'].name,'spectrum':spectra,'mz':mz,'intensity':intensity})\n",
    "    data = pd.DataFrame(data)\n",
    "    data[['name','mz','intensity']].to_csv(os.path.join(output_dir,'spectra_1pct_450cut.csv'),index=None)\n",
    "    # to look at it type this:\n",
    "    data.head(20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
