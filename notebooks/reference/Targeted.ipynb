{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "The next code block sets parameters that are used throughout the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# one of 'positive' or 'negative'\n",
    "polarity = 'positive'\n",
    "\n",
    "# one of 'ISTDsEtc' or 'FinalEMA-HILIC'\n",
    "output_type = 'FinalEMA-HILIC'\n",
    "\n",
    "# an integer, increment if you need to redo your analysis\n",
    "# will be appended to your username to create analysis_id\n",
    "analysis_number = 0\n",
    "\n",
    "# experiment ID that must match the parent folder containing the LCMS output files\n",
    "# An example experiment ID is '20201116_JGI-AK_LH_506489_SoilWarm_final_QE-HF_HILICZ_USHXG01530'\n",
    "experiment = 'REPLACE ME'\n",
    "\n",
    "# Exclude files with names containing any of the substrings in this list. Eg., ['peas', 'beans']\n",
    "exclude_files = []\n",
    "\n",
    "# Exclude groups with names containing any of the substrings in this list.\n",
    "# 'POS' or 'NEG' will be auto-appended later, so you shouldn't use them here.\n",
    "exclude_groups = ['QC','InjBl']\n",
    "\n",
    "# thresholds for filtering out compounds with weak MS1 signals\n",
    "num_points_passing = 5\n",
    "peak_height_passing = 4e5\n",
    "\n",
    "# include MSMS fragment ions in the output documents?\n",
    "export_msms_fragment_ions = False\n",
    "\n",
    "# list of substrings that will group together when creating groups\n",
    "# this provides additional grouping beyond the default grouping on field #12\n",
    "groups_controlled_vocab = ['QC','InjBl','ISTD']\n",
    "\n",
    "# list of tuples contain string with color name and substring pattern.\n",
    "# Lines in the EIC plot will be colored by the first substring pattern\n",
    "# that has a match within the name of the hdf5_file. The order they are\n",
    "# listed in your list is the order they are displayed in the overlays\n",
    "# (first is front, last is back). Named colors available in matplotlib\n",
    "# are here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "# or use hexadecimal values '#000000'. Lines default to black.\n",
    "rt_adjuster_color_list = [('red','ExCtrl'),                                                     \n",
    "                          ('green','TxCtrl'),\n",
    "                          ('blue','InjBl')]\n",
    "\n",
    "# The rest of this block contains project independent parameters\n",
    "\n",
    "# Full path to the directory where you have cloned the metatlas git repo.\n",
    "# If you ran the 'git clone ...' command in your home directory on Cori, \n",
    "# then you'll want '/global/homes/FIRST-INITIAL-OF-USERNAME/USERNAME/metatlas'\n",
    "# where the uppercase letters are replaced based on your NERSC username.\n",
    "metatlas_repo_path = '/global/homes/FIRST-INITIAL-OF-USERNAME/USERNAME/metatlas'\n",
    "\n",
    "\n",
    "# Full path to the directory where you want this notebook to store data.\n",
    "# A subdirectory will be auto created within this directory for each project.\n",
    "# You can place this anywhere on cori's filesystem, but placing it within your\n",
    "# global home directory is recommended so that you do not need to worry about\n",
    "# your data being purged. Each project will take on the order of 100 MB.\n",
    "project_directory = '/global/homes/FIRST-INITIAL-OF-USERNAME/USERNAME/metabolomics_projects'\n",
    "\n",
    "# maximum number of CPUs to use\n",
    "# when running on jupyter.nersc.gov, you are not allowed to set this above 4\n",
    "max_cpus = 4\n",
    "\n",
    "# Threshold for how much status information metatlas functions print in the notebook\n",
    "# levels are 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'\n",
    "log_level = 'INFO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import sys, os\n",
    "os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "sys.path.insert(0, metatlas_repo_path)\n",
    "try:\n",
    "    import dataset\n",
    "except ModuleNotFoundError:\n",
    "    print('Could not find dataset module. Please check that the kernel is set to \"metatlas py3\".')\n",
    "    raise ValueError('Invalid kernel setting in Jupyter Notebook.')\n",
    "if not os.path.exists(metatlas_repo_path):\n",
    "    print('Directory set for metatlas_repo_path parameter does not exists.')\n",
    "    raise ValueError('Invalid metatlas_repo_path parameter in Jupyter Notebook.')\n",
    "try:\n",
    "    from metatlas.tools import fastanalysis as fa\n",
    "except ModuleNotFoundError:\n",
    "    print('Could not find metatlas module. In the Parameters block, please check the value of metatlas_repo_path.')\n",
    "    raise ValueError('Invalid metatlas_repo_path parameter in Jupyter Notebook.')\n",
    "from metatlas.plots import dill2plots as dp\n",
    "from metatlas.io import metatlas_get_data_helper_fun as ma_data\n",
    "from metatlas.datastructures import metatlas_objects as metob\n",
    "from metatlas.datastructures import metatlas_dataset as mads\n",
    "from metatlas.tools.logging import activate_logging\n",
    "\n",
    "import getpass\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "if polarity not in ['positive', 'negative']:\n",
    "    raise ValueError('Parameter polarity is not one of \"positive\" or \"negative\".')\n",
    "\n",
    "if output_type == 'ISTDsEtc':\n",
    "    pass\n",
    "elif output_type == 'FinalEMA-HILIC':\n",
    "    num_data_points_passing = 5\n",
    "    peak_height_passing = 4e5\n",
    "else:\n",
    "    raise ValueError('Parameter output_type is not one of \"ISTDsEtc\" or \"FinalEMA-HILIC\".')\n",
    "\n",
    "if experiment == 'Replace me':\n",
    "    raise ValueError('Parameter experiment has not been set.')\n",
    "if len(experiment.split('_')) != 9:\n",
    "    raise ValueError('Parameter experiment does contain 9 fields when split on \"_\".')\n",
    "\n",
    "activate_logging(console_level=log_level)\n",
    "logger = logging.getLogger('metatlas.jupyter')\n",
    "\n",
    "username = getpass.getuser()\n",
    "analysis_id = f\"{username}{analysis_number}\"\n",
    "output_dir = os.path.join(project_directory, experiment, analysis_id, output_type)\n",
    "short_experiment_analysis_id = experiment.split('_')[0]+'_'+experiment.split('_')[3]+'_'+analysis_id\n",
    "\n",
    "os.makedirs(project_directory, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# set notebook to have minimal side margins\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "logger.info(\"experiment=%s, analysis_id=%s, short_experiment_analysis_id=%s\", experiment, analysis_id, short_experiment_analysis_id)\n",
    "logger.info(\"output_dir=%s\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCMS filenaming convention\n",
    "\n",
    "### You must assign your raw files into experimental groups for analysis.  These are used for downstream statistics and for selection of specific groups for filtering to subsets of files for analysis (Ex. just pos or just neg).\n",
    "\n",
    "The groups are created from common file headers and the unique group names. The convention our lab group uses for filenames is as follows: \n",
    "***\n",
    "DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ_SAMPLENUMBER_ SAMPLEGROUP_REP_OPTIONAL_SEQ \n",
    "\n",
    "Ex.:20180105_SK_AD_ENIGMA_PseudoInt_R2ADec2017_QE119_50454_123456_POS_MSMS_001_Psyringae-R2A-30C-20hr_Rep01_NA_Seq001.raw\n",
    "***\n",
    "The common header consists of the fields 0-10: DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ \n",
    "\n",
    "The sample group name is commonly field # 12 (between underscore 11 and 12) -0 indexed-\n",
    "# Find your files\n",
    "1. On the first line of the block below, set the 'experiment' and 'name' variables to find your files.  These fields require wildcards for partial string searches\n",
    "2. 'Experiment' is the folder name within global/project/projectdirs/metatlas/raw_data, that will be emailed to you when the files are uploaded to NERSC.  You can also look in the raw_data directory for the NERSC user who uploaded your files; your experiment folder should be in there.\n",
    "3. 'name' is string that will match a subset of your files within that folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dp.get_metatlas_files(experiment = experiment,name = '%',most_recent = True)\n",
    "df = metob.to_dataframe(files)\n",
    "logger.info(\"Number of LCMS output files matching '%s' is: %d.\", experiment, len(files))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Groups\n",
    "This will attempt to create groups in an automated fashion (rather than filling out a spreadsheet with a list of files and group names).  If your files are all in one folder at nersc, you can use this options.  If not, use option B below.\n",
    "\n",
    "A long group name consisting of the common header + either controlled vocab value or field #12 along with a short group name (just controlled vocab or field #12) will be stored in a local variable.  The short group names can be used on plots.\n",
    "\n",
    "\n",
    "1. STEP 1: View the groups\n",
    "    1. Pick an experiment folder to look for files in on the metob.retrieve function\n",
    "    2. Enter controlled vocabulary for control files to put select files into groups when control string may be in a different field (not #12) or as a randomly placed substring within a field (ex. if 'InjBl' is included in your controlled vocab list, files like _InjBl-MeOH_ and _StartInjBl_ will group together)\n",
    "    3. If your group name is not between _ 11 and 12 you can adjust those values in the split commands below.  All other (non-controlledvocab) groups will be created from that field.\n",
    "2. STEP 2: Create the groups variable after checking the output from STEP 1\n",
    "3. STEP 3: <br />\n",
    "    Option A: If everything looks fine the group names and short names, Store groups once you know you have files in correct groups by running and checking the output of STEPS 1 and 2.<br />\n",
    "    Option B (optional): If you would like to edit the groups, uncomment the options B-I and B-II. Run Option B-I to export a prefilled tab infosheet. Edit the file and then run Option B-II to import the new groups and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: View the groups\n",
    "files = dp.get_metatlas_files(experiment = experiment,name = '%',most_recent = True)\n",
    "file_dict = {}\n",
    "groups_dict = {}\n",
    "for f in files:\n",
    "    if not any(map(f.name.__contains__, exclude_files)):\n",
    "        k = f.name.split('.')[0]\n",
    "        #     get index if any controlled vocab in filename\n",
    "        indices = [i for i, s in enumerate(groups_controlled_vocab) if s.lower() in k.lower()]\n",
    "        prefix = '_'.join(k.split('_')[:11])\n",
    "        if len(indices)>0:\n",
    "            short_name = groups_controlled_vocab[indices[0]].lstrip('_')\n",
    "            group_name = '%s_%s_%s'%(prefix,analysis_id,short_name)\n",
    "            short_name = k.split('_')[9]+'_'+short_name # Prepending POL to short_name\n",
    "        else:\n",
    "            short_name = k.split('_')[12]\n",
    "            group_name = '%s_%s_%s'%(prefix,analysis_id,short_name)\n",
    "            short_name = k.split('_')[9]+'_'+k.split('_')[12]  # Prepending POL to short_name\n",
    "        file_dict[k] = {'file':f,'group':group_name,'short_name':short_name}\n",
    "        groups_dict[group_name] = {'items':[],'name':group_name,'short_name':short_name}\n",
    "df = pd.DataFrame(file_dict).T\n",
    "df.index.name = 'filename'\n",
    "df.reset_index(inplace=True)#['group'].unique()\n",
    "df.drop(columns=['file'],inplace=True)\n",
    "for ug in groups_dict.keys():\n",
    "    for file_key,file_value in file_dict.items():\n",
    "        if file_value['group'] == ug:\n",
    "            groups_dict[ug]['items'].append(file_value['file'])\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: create the groups variable, if the above looks OK\n",
    "groups = []\n",
    "for group_key,group_values in groups_dict.items():\n",
    "    g = metob.Group(name=group_key,items=group_values['items'],short_name=group_values['short_name'])\n",
    "    groups.append(g)        \n",
    "    for item in g.items:\n",
    "        print(g.name,g.short_name,item.name)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 Option A: store the groups variable content in the DB (currently only the long group name is stored)\n",
    "metob.store(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data frame of short filenames and samplenames\n",
    "Uncomment the below 2 blocks to make short file names and smaple names.<br>\n",
    "This creates a dataframe and a csv file which can be edited, exported and imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make short_filename and short_samplename \n",
    "files = metob.retrieve('lcmsruns',experiment=experiment,username='*')\n",
    "short_filename_delim_ids = [0,2,4,5,7,9,14]\n",
    "short_samplename_delim_ids = [9,12,13,14]\n",
    "short_names_df = pd.DataFrame(columns=['sample_treatment','short_filename','short_samplename'])\n",
    "ctr = 0\n",
    "for f in files:\n",
    "    short_filename = []\n",
    "    short_samplename = []\n",
    "    tokens = f.name.split('.')[0].split('_')\n",
    "    for id in short_filename_delim_ids:\n",
    "        short_filename.append(str(tokens[id]))\n",
    "    for id in short_samplename_delim_ids:\n",
    "        short_samplename.append(str(tokens[id]))\n",
    "    short_filename = \"_\".join(short_filename)\n",
    "    short_samplename = \"_\".join(short_samplename)\n",
    "    short_names_df.loc[ctr, 'full_filename'] = f.name.split('.')[0]\n",
    "    short_names_df.loc[ctr, 'sample_treatment'] = str(tokens[12]) # delim 12\n",
    "    short_names_df.loc[ctr, 'short_filename'] = short_filename\n",
    "    short_names_df.loc[ctr, 'short_samplename'] = short_samplename\n",
    "    short_names_df.loc[ctr, 'last_modified'] = pd.to_datetime(f.last_modified,unit='s')\n",
    "    ctr +=1\n",
    "short_names_df.sort_values(by='last_modified', inplace=True)\n",
    "short_names_df.drop(columns=['last_modified'], inplace=True)\n",
    "short_names_df.drop_duplicates(subset=['full_filename'], keep='last', inplace=True)\n",
    "short_names_df.set_index('full_filename', inplace=True)\n",
    "short_names_df.to_csv(os.path.join(output_dir, 'short_names.csv'), sep=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select groups of files to operate on\n",
    "\n",
    "Here, you will assign your database groups to a local variable which will be used downstream in the notebook for analyzing your data with an atlas.\n",
    "\n",
    "1. in block below, fill out the fields for name, include_list and exclude_list using text strings from the group names you created in the previous step.  The include/exlcude lists do not need wildcards.  Name is a string unique to all of your groups (ex. fields 0-11 of your filenames)\n",
    "\n",
    "### Typically, you will run one polarity at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_groups.append('NEG' if polarity=='positive' else 'POS')\n",
    "groups = dp.select_groups_for_analysis(name = experiment+'%',\n",
    "                                       most_recent = True,\n",
    "                                       remove_empty = True,\n",
    "                                       include_list = [], exclude_list = exclude_groups)\n",
    "print(\"sorted groups\")\n",
    "groups = sorted(groups, key=lambda x: x.name)\n",
    "for i,a in enumerate(groups):\n",
    "    print(i, a.name)\n",
    "metob.to_dataframe(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Atlas to use\n",
    "\n",
    "1. The first block will retrieve a list of atlases matching the 'name' string that you enter.  Also, you must enter your username.\n",
    "2. The next block will select one from the list, using the index number.  Make sure to enter the index number for the atlas you want to use for your analysis by setting in this line: atlas_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_query = f\"%_{polarity[:3].upper()}_{short_experiment_analysis_id}%\"\n",
    "atlases = metob.retrieve('atlases', name=f\"%_{polarity[:3].upper()}_{short_experiment_analysis_id}%\", username=username)\n",
    "for i,a in enumerate(atlases):\n",
    "    print(i,a.name,pd.to_datetime(a.last_modified,unit='s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_idx = 0\n",
    "metatlas_dataset = mads.MetatlasDataset(atlases[atlas_idx], groups, max_cpus=max_cpus)\n",
    "ma_data.make_data_sources_tables(groups, metatlas_dataset.atlas, output_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Filter atlas for compounds with no or low signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metatlas_dataset.filter_compounds_by_signal(num_points=num_points_passing, peak_height=peak_height_passing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = dp.adjust_rt_for_selected_compound(metatlas_dataset, msms_hits=metatlas_dataset.hits,\n",
    "                                       color_me=rt_adjuster_color_list,\n",
    "                                       compound_idx=0, alpha=0.5, width=18, height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results files\n",
    "### Filter out compounds with ms1_notes of 'remove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metatlas_dataset.filter_compounds_ms1_notes_remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Atlas to a Spreadsheet\n",
    "\n",
    "The peak flags that you set and selected from the rt adjuster radio buttons will be saved in a column called id_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_atlas_filename = os.path.join(output_dir, f\"{polarity[:3].upper()}_{metatlas_dataset.atlas.name}_export\")\n",
    "atlas_identifications = dp.export_atlas_to_spreadsheet(metatlas_dataset.atlas, export_atlas_filename)\n",
    "logger.info(\"Exported atlas to file: %s.\", export_atlas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS match scores, stats sheets, and final identification table\n",
    "\n",
    "This block creates a number of files:\n",
    "\n",
    "1. compound_scores.csv\n",
    "2. stats_table.tab\n",
    "3. filtered and unfiltered peak heights, areas, msms scores, mz centroid, mz ppm error, num of fragment matches, rt delta, rt peak\n",
    "4. final identification sheet that is formatted for use as a supplemental table for manuscript submission.  You will need to manually complete some columns.  Please discuss with Ben, Katherine, Daniel or Suzie before using for the first time.\n",
    "\n",
    "THe kwargs below will set the filtering points for the parameters indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'min_intensity': 1e4,   # strict = 1e5, loose = 1e3\n",
    "          'rt_tolerance': .5,    #>= shift of median RT across all files for given compound to reference\n",
    "          'mz_tolerance': 20,      # strict = 5, loose = 25; >= ppm of median mz across all files for given compound relative to reference\n",
    "          'min_msms_score': .6, 'allow_no_msms': True,     # strict = 0.6, loose = 0.3 <= highest compound dot-product score across all files for given compound relative to reference\n",
    "          'min_num_frag_matches': 1, 'min_relative_frag_intensity': .001}   # strict = 3 and 0.1, loose = 1, 0.01 number of matching mzs when calculating max_msms_score and ratio of second highest to first highest intensity of matching sample mzs\n",
    "scores_df = fa.make_scores_df(metatlas_dataset, metatlas_dataset.hits)\n",
    "scores_df['passing'] = fa.test_scores_df(scores_df, **kwargs)\n",
    "\n",
    "pass_atlas_df, fail_atlas_df, pass_dataset, fail_dataset = fa.filter_atlas_and_dataset(scores_df, metatlas_dataset.atlas_df, metatlas_dataset, column='passing')\n",
    "\n",
    "fa.make_stats_table(input_dataset=metatlas_dataset, msms_hits=metatlas_dataset.hits, output_loc=output_dir, min_peak_height=1e5, use_labels=True, min_msms_score=0.01, min_num_frag_matches=1, include_lcmsruns=[], exclude_lcmsruns=['QC'], polarity=polarity[:3].upper())\n",
    "scores_df.to_csv(os.path.join(output_dir,'stats_tables', polarity[:3].upper()+'_compound_scores.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export EIC chromatograms as individual pdfs for each compound\n",
    "\n",
    "1.  There are three options for formatting your EIC output using the \"group =\" line below:\n",
    "    1. 'page' will print each sample group on a new page of a pdf file\n",
    "    2. 'index' will label each group with a letter\n",
    "    3. None will print all of the groups on one page with very small subplot labels\n",
    "2. The Y axis scale can be shared across all files using share_y = True or set to the max within each file using share_y = False\n",
    "3. To use short names for plots, short_names_df should be provided as input. Additionally the header column to be used for short names should be provided as follows (short_names_df=short_names_df, short_names_header='short_samplename'). Header options are sample_treatment, short_filename, short_samplename. These are optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = 'index' # 'page' or 'index' or None\n",
    "save = True\n",
    "share_y = True\n",
    "\n",
    "dp.make_chromatograms(input_dataset=metatlas_dataset, include_lcmsruns=[], exclude_lcmsruns=['InjBl','QC','Blank','blank'], group=group, share_y=share_y, save=save, output_loc=output_dir, short_names_df=short_names_df, short_names_header='short_samplename', polarity=polarity[:3].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS mirror plots as individual pdfs for each compound\n",
    "\n",
    "1. use_labels = True will use the compound names you provided in your atlas, if you set it to false, the compounds will be named with the first synonym available from pubchem which could be a common name, iupac name, cas number, vendor part number, etc. \n",
    "2.  The include and exclude lists will match partial strings in filenames, do not use wildcards.\n",
    "3. If short_names_df is provided as input, short_samplename is used for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dp.make_identification_figure_v2(input_dataset=metatlas_dataset, msms_hits=metatlas_dataset.hits, use_labels=True, include_lcmsruns=[], exclude_lcmsruns=['InjBl', 'QC', 'Blank', 'blank'], output_loc=output_dir,  short_names_df=short_names_df, polarity=polarity[:3].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sheets\n",
    "1. To include short names in the output, short_names_df should be provided as input to make_output_dataframe. \n",
    "2. ylabel is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = partial(dp.make_output_dataframe, input_dataset=metatlas_dataset, include_lcmsruns=[], exclude_lcmsruns=[], output_loc=os.path.join(output_dir,polarity[:3].upper()+'_data_sheets'), short_names_df=short_names_df, polarity=polarity[:3].upper(), use_labels=True)\n",
    "peak_height = output_dataframe(fieldname='peak_height')\n",
    "peak_area = output_dataframe(fieldname='peak_area')\n",
    "mz_peak = output_dataframe(fieldname='mz_peak')\n",
    "rt_peak = output_dataframe(fieldname='rt_peak')\n",
    "mz_centroid = output_dataframe(fieldname='mz_centroid')\n",
    "rt_centroid = output_dataframe(fieldname='rt_centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.make_boxplot_plots(rt_peak, output_loc=os.path.join(output_dir, polarity[:3].upper()+'_boxplot_rt_peak'), ylabel=\"RT Peak\")\n",
    "dp.make_boxplot_plots(peak_height, output_loc=os.path.join(output_dir, polarity[:3].upper()+'_boxplot_peak_height'), ylabel=\"Peak Height\")\n",
    "dp.make_boxplot_plots(mz_centroid, output_loc=os.path.join(output_dir, polarity[:3].upper()+'_boxplot_mz_centroid'), ylabel=\"MZ Centroid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS fragment Ions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_msms_fragment_ions:\n",
    "    intensity_fraction = 0.01\n",
    "    min_mz = 450.0 #minimum m/z to export in msms\n",
    "    max_mz = -40.0 # distance from precurosor to export (0.5 is a good number. crazy people use negative numbers)\n",
    "    scale_intensity = True\n",
    "    data = []\n",
    "    for compound_index in range(len(metatlas_dataset[0])):\n",
    "        max_intensity = 0\n",
    "        d = {}\n",
    "        for file_index in range(len(metatlas_dataset)):\n",
    "            try:\n",
    "                pk_idx = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['precursor_intensity'].argmax()\n",
    "                pk = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['precursor_intensity'][pk_idx]\n",
    "                precursor_mz = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['precursor_MZ'][pk_idx]\n",
    "                rt = metatlas_dataset[file_index][compound_index]['data']['msms']['data']['rt'][pk_idx]\n",
    "                if (pk>max_intensity) & (rt>metatlas_dataset[file_index][compound_index]['identification'].rt_references[-1].rt_min) & (rt<metatlas_dataset[file_index][compound_index]['identification'].rt_references[-1].rt_max):\n",
    "                    good_index = file_index\n",
    "                    max_intensity = pk\n",
    "                    final_mz = precursor_mz #save this for filtering below\n",
    "            except:\n",
    "                pass\n",
    "    #     print(compound_index,good_index,max_intensity)\n",
    "        if max_intensity>0:\n",
    "            msms = metatlas_dataset[good_index][compound_index]['data']['msms']['data']\n",
    "            idx = np.argwhere(msms['precursor_intensity']==max_intensity).flatten()\n",
    "            mz = msms['mz'][idx]\n",
    "            intensity = msms['i'][idx]\n",
    "            max_msms_intensity = intensity.max()\n",
    "            cutoff = intensity_fraction * max_msms_intensity\n",
    "            conditions = (intensity>cutoff) & (mz>min_mz) & (mz<(final_mz+max_mz))\n",
    "            if sum(conditions)>0:\n",
    "                keep_idx = np.argwhere(conditions).flatten()\n",
    "                mz = str(['%.2f'%x for x in list(mz[keep_idx])]).replace('\\'','')\n",
    "                if scale_intensity==True:\n",
    "                    intensity = intensity / intensity.max()\n",
    "                    intensity = intensity * 1e5\n",
    "                    intensity = intensity.astype(int)\n",
    "                intensity = str(['%d'%x for x in list(intensity[keep_idx])]).replace('\\'','')\n",
    "                spectra = str([mz,intensity]).replace('\\'','')\n",
    "            else:\n",
    "                mz = None\n",
    "                intensity = None\n",
    "                spectra = None\n",
    "        else:\n",
    "            mz = None\n",
    "            intensity = None\n",
    "            spectra = None\n",
    "        data.append({'name':metatlas_dataset[file_index][compound_index]['identification'].name,'spectrum':spectra,'mz':mz,'intensity':intensity})\n",
    "    data = pd.DataFrame(data)\n",
    "    data[['name','mz','intensity']].to_csv(os.path.join(output_dir,'spectra_1pct_450cut.csv'),index=None)\n",
    "    # to look at it type this:\n",
    "    data.head(20)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
