{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetAtlas analysis workflow\n",
    "\n",
    "This notebook is for performing a targeted metabolomics analysis on your raw data files.  In order for this to work, you data files must be uploaded to the nersc raw data directory with appropriate permissions.  A fileconverter runs at nersc to convert as follows:\n",
    ".raw > .mzml > .h5 > pactolus.gz  This conversion must happen before you can proceed with data analysis.  A Northen Lab staff member can help you transfer and convert your files if they are not already at nersc.\n",
    "***\n",
    "/global/project/projectdirs/metatlas/raw_data/SUBFOLDER\n",
    "***\n",
    "\n",
    "A targeted analysis requires a list of compounds to search for in your sample files.  Thus, at a minimum, you must have some information on their m/z ratio in order to get started.  The inputs for this notebook, include the location of your raw files, and an atlas listing out these compounds.  \n",
    "\n",
    "Our lab has analyzed thousands of compounds using standardized LCMS methods.  You may use one of these libraries to create an atlas for your sample analysis, assuming you used one of the same standard LCMS methods.\n",
    "\n",
    "Analysis steps\n",
    "\n",
    "<ol>\n",
    "<li>Create your atlas csv external to jupyter: typically you will either have a custom atlas or generate an atlas from one of our EMA libraries.  These can be found in the shared google drive, under atlases.</li>\n",
    "<li>Run through with your initial atlas</li>\n",
    "<li>Use the RT adjuster to mark the quality of the peaks and MSMS matches, and also adjust your RT bounds</li>\n",
    "<li>Reload your atlas from the \"get atlas\" block</li>\n",
    "<li>Export your atlas, filter out the compounds marked under id notes as remove.  Save and upload your new atlas</li>\n",
    "<li>Repeat the analysis using your filtered atlas.</li>\n",
    "    <li>On your final export, make sure you are excluding system blanks and QC files in your output.  If you have extraction blanks you should include those.</li>\n",
    "    </ol>\n",
    "    \n",
    "There is a database that will store your atlas and registered run files.  During the analysis process you will pull these as local variables. These are used to pull out EIC and MSMS data which is stored in a variable called metatlas_dataset.  All output files are generated from that.  It is a local variable so is not stored in the db - thus if your kernel dies partway through, you need to rerun through the notebook to regenerate that variable.\n",
    "\n",
    "IMPORTANT: Anytime you adjust your RTs in the interactive plot, you need to retreive your atlas again before exporting any files.  The changes are stored in the db and the exports are made from locally stored variables/dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# index\n",
    "1. Import python packages\n",
    "2. Create groups\n",
    "3. Select groups\n",
    "4. Create Atlases\n",
    "5. Select Atlas\n",
    "6. Annotate data\n",
    "7. Correct RT bounds\n",
    "8. Export results\n",
    "9. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful tips\n",
    "\n",
    "1. When entering search strings, use % for wildcard: %peas will return \"positive_peas\" and \"positive_greenpeas\", but not \"positive_greenpeas_HILIC\", for that use %peas%, you can also put multiple strings via %pos%peas%\n",
    "2. Set your kernel to \"Metatlas Targeted\".  If not sure how, check w Ben or Will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=invalid-name,missing-module-docstring\n",
    "\n",
    "# full name of input atlas to load from database or name to give atlas loaded from CSV file\n",
    "source_atlas = None\n",
    "\n",
    "# full path to atlas CSV file or None if loading an atlas from the database\n",
    "csv_atlas_file_name = None\n",
    "\n",
    "# one of 'positive' or 'negative'\n",
    "polarity = \"positive\"\n",
    "\n",
    "# one of 'data_QC', 'ISTDsEtc', 'FinalEMA-HILIC', or 'other'\n",
    "output_type = \"FinalEMA-HILIC\"\n",
    "\n",
    "# an integer, increment if you need to redo your analysis\n",
    "# will be appended to your username to create analysis_id\n",
    "analysis_number = 0\n",
    "\n",
    "# experiment ID that must match the parent folder containing the LCMS output files\n",
    "# An example experiment ID is '20201116_JGI-AK_LH_506489_SoilWarm_final_QE-HF_HILICZ_USHXG01530'\n",
    "experiment = \"REPLACE ME\"\n",
    "\n",
    "# A list of experiment IDs to pull LCMS runs from\n",
    "# if None, then the value of the \"experiment\" variable will be used\n",
    "run_batches = None\n",
    "\n",
    "# Exclude files with names containing any of the substrings in this list. Eg., ['peas', 'beans']\n",
    "exclude_files = []\n",
    "\n",
    "# Exclude groups with names containing any of the substrings in this list.\n",
    "# 'POS' or 'NEG' will be auto-appended later, so you shouldn't use them here.\n",
    "exclude_groups = [\"QC\", \"InjBl\"]\n",
    "\n",
    "# thresholds for filtering out compounds with weak MS1 signals\n",
    "num_points = 6\n",
    "peak_height = 4e5\n",
    "\n",
    "# if loading an atlas from CSV, assign all entries this mz_tolerance (in ppm)\n",
    "mz_tolerance = 5\n",
    "\n",
    "# file name of the file containing the reference MSMS data\n",
    "msms_refs_file_name = '/global/project/projectdirs/metatlas/projects/spectral_libraries/msms_refs_v3.tab'\n",
    "\n",
    "# list of substrings that will group together when creating groups\n",
    "# this provides additional grouping beyond the default grouping on field #12\n",
    "groups_controlled_vocab = [\"QC\", \"InjBl\", \"ISTD\"]\n",
    "\n",
    "# list of tuples contain string with color name and substring pattern.\n",
    "# Lines in the EIC plot will be colored by the first substring pattern\n",
    "# that has a match within the name of the hdf5_file. The order they are\n",
    "# listed in your list is the order they are displayed in the overlays\n",
    "# (first is front, last is back). Named colors available in matplotlib\n",
    "# are here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "# or use hexadecimal values '#000000'. Lines default to black.\n",
    "line_colors = [('green','ISTD'), ('red','NoD2O')]\n",
    "\n",
    "# Setting this to True will remove the cache of MSMS hits\n",
    "# if you don't see MSMS data for any of your compounds in RT adjuster GUI,\n",
    "# then you might want to try settings this to True. However, it will\n",
    "# make your notebook take significantly longer to run.\n",
    "# The cache is per experiment, so clearing the cache will impact other\n",
    "# notebooks for this same experiment.\n",
    "clear_cache = False\n",
    "\n",
    "# subfolder to be created within project_directly to hold data from this analysis\n",
    "output_subfolder='Anuvia/Pilot4_AgriSoils_pooled/HILICZ'\n",
    "\n",
    "# The rest of this block contains project independent parameters\n",
    "\n",
    "# to use an older version of the metatlas source code, set this to a commit id,\n",
    "# branch name, or tag. If None, then use the the \"main\" branch.\n",
    "source_code_version_id = None\n",
    "\n",
    "# Full path to the directory where you want this notebook to store data.\n",
    "# A subdirectory will be auto created within this directory for each project.\n",
    "# You can place this anywhere on cori's filesystem, but placing it within your\n",
    "# global home directory is recommended so that you do not need to worry about\n",
    "# your data being purged. Each project will take on the order of 100 MB.\n",
    "project_directory='/global/homes/k/kblouie/metabolomics/Plots/'\n",
    "\n",
    "# ID from Google Drive URL for base output folder .\n",
    "# The default value is the ID that corresponds to 'JGI_Metabolomics_Projects'.\n",
    "google_folder = \"0B-ZDcHbPi-aqZzE5V3hOZFc0dms\"\n",
    "\n",
    "# maximum number of CPUs to use\n",
    "# when running on jupyter.nersc.gov, you are not allowed to set this above 4\n",
    "max_cpus = 4\n",
    "\n",
    "# Threshold for how much status information metatlas functions print in the notebook\n",
    "# levels are 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'\n",
    "log_level = \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=wrong-import-position,import-error,missing-class-docstring\n",
    "import logging  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"metatlas.jupyter\")\n",
    "kernel_def = \"\"\"{\"argv\":[\"shifter\",\"--entrypoint\",\"--image=doejgi/metatlas_shifter:latest\",\"/usr/local/bin/python\",\"-m\",\n",
    "                 \"ipykernel_launcher\",\"-f\",\"{connection_file}\"],\"display_name\": \"Metatlas Targeted\",\"language\": \"python\",\n",
    "                 \"metadata\": { \"debugger\": true }}\"\"\"\n",
    "kernel_file_name = Path.home() / \".local\" / \"share\" / \"jupyter\" / \"kernels\" / \"metatlas-targeted\" / \"kernel.json\"\n",
    "kernel_file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "try:\n",
    "    with Path(\"/metatlas_image_version\").open(mode=\"r\", encoding=\"utf-8\") as f:\n",
    "        version = f.readlines()[0].rstrip()\n",
    "except FileNotFoundError:\n",
    "    version = \"0\"\n",
    "try:\n",
    "    has_root_kernel = Path('/root/.local/share/jupyter/kernels/papermill/kernel.json').is_file()\n",
    "except PermissionError:\n",
    "    has_root_kernel = False\n",
    "if not (version == \"1\" and (kernel_file_name.is_file() or has_root_kernel)):\n",
    "    with kernel_file_name.open(mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(kernel_def)\n",
    "    logger.critical(\"CRITICAL: Notebook kernel has been updated. Restart kernel and re-run notebook.\")\n",
    "    raise StopExecution\n",
    "try:\n",
    "    from metatlas.tools import notebook  # noqa: E402\n",
    "except ImportError as err:\n",
    "    logger.critical('CRITICAL: Please check that the kernel is set to \"Metatlas Targeted\".')\n",
    "    raise StopExecution from err\n",
    "notebook.setup(log_level, source_code_version_id)\n",
    "\n",
    "import getpass\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from metatlas.tools import fastanalysis as fa\n",
    "from metatlas.plots import dill2plots as dp\n",
    "from metatlas.io import metatlas_get_data_helper_fun as ma_data\n",
    "from metatlas.datastructures import metatlas_objects as metob\n",
    "from metatlas.datastructures.analysis_identifiers import AnalysisIdentifiers\n",
    "from metatlas.datastructures import metatlas_dataset as mads\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "assert source_atlas is not None\n",
    "run_batches = run_batches if run_batches is not None else [experiment] \n",
    "%matplotlib widget\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set atlas, project and output directories from your nersc home directory\n",
    "\n",
    "STEP 1\n",
    "1. Create a project folder name for this analysis by replacing the PROJECTDIRECTORY string text in red below.  Make sure to update the rest of the direcory to point to your home directory.  The pwd block will print out the directory where this jupyter notebook is stored.\n",
    "2. Create a subdirectory name for the output, each run through you may want to create a new output folder.\n",
    "3. When you run the block the folders will be created in your home directory.  If the directory already exists, the block will just set the path for use with future code blocks.\n",
    "\n",
    "STEP 2\n",
    "1.  Enter the nersc path where you have stored your atlas files using one of the two optional lines below:\n",
    "    1. Set a custom path\n",
    "    2. Use the project and output subdirectory from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1\n",
    "output_dir = os.path.join(project_directory,output_subfolder)\n",
    "os.makedirs(project_directory, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Create Groups (named variables that hold your replicates of each sample)\n",
    "\n",
    "### You must assign your raw files into experimental groups for analysis.  These are used for downstream statistics and for selection of specific groups for filtering to subsets of files for analysis (Ex. just pos or just neg).\n",
    "\n",
    "The groups are created from common file headers and the unique group names. The convention our lab group uses for filenames is as follows: \n",
    "***\n",
    "DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ_SAMPLENUMBER_ SAMPLEGROUP_REP_OPTIONAL_SEQ \n",
    "\n",
    "Ex.:20180105_SK_AD_ENIGMA_PseudoInt_R2ADec2017_QE119_50454_123456_POS_MSMS_001_Psyringae-R2A-30C-20hr_Rep01_NA_Seq001.raw\n",
    "***\n",
    "The common header consists of the fields 0-10: DATE_NORTHENLABINITIALS_COLLABINITIALS_PROJ_EXP_SAMPSET_SYSTEM_COLUMN-method_SERIAL_POL_ACQ \n",
    "\n",
    "The sample group name is commonly field # 12 (between underscore 11 and 12) -0 indexed-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find your files\n",
    "1. On the first line of the block below, set the 'experiment' and 'name' variables to find your files.  These fields require wildcards for partial string searches\n",
    "2. 'Experiment' is the folder name within global/project/projectdirs/metatlas/raw_data, that will be emailed to you when the files are uploaded to NERSC.  You can also look in the raw_data directory for the NERSC user who uploaded your files; your experiment folder should be in there.\n",
    "3. 'name' is string that will match a subset of your files within that folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dp.get_metatlas_files(experiment=[f\"{b}%\" for b in run_batches], name=\"%\", most_recent=True)\n",
    "df = metob.to_dataframe(files)\n",
    "df[['experiment','name','username','acquisition_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION A: Automated Group Maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will attempt to create groups in an automated fashion (rather than filling out a spreadsheet with a list of files and group names).  If your files are all in one folder at nersc, you can use this options.  If not, use option B below.\n",
    "\n",
    "A long group name consisting of the common header + either controlled vocab value or field #12 along with a short group name (just controlled vocab or field #12) will be stored in a local variable.  The short group names can be used on plots.\n",
    "\n",
    "\n",
    "1. STEP 1: View the groups\n",
    "    1. Pick an experiment folder to look for files in on the metob.retrieve function\n",
    "    2. Enter controlled vocabulary for control files to put select files into groups when control string may be in a different field (not #12) or as a randomly placed substring within a field (ex. if 'InjBl' is included in your controlled vocab list, files like _InjBl-MeOH_ and _StartInjBl_ will group together)\n",
    "    3. If your group name is not between _ 11 and 12 you can adjust those values in the split commands below.  All other (non-controlledvocab) groups will be created from that field.\n",
    "2. STEP 2: Create the groups variable after checking the output from STEP 1\n",
    "3. STEP 3: <br />\n",
    "    Option A: If everything looks fine the group names and short names, Store groups once you know you have files in correct groups by running and checking the output of STEPS 1 and 2.<br />\n",
    "    Option B (optional): If you would like to edit the groups, uncomment the options B-I and B-II. Run Option B-I to export a prefilled tab infosheet. Edit the file and then run Option B-II to import the new groups and save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: View the groups\n",
    "\n",
    "files = dp.get_metatlas_files(experiment=[f\"{b}%\" for b in run_batches], name=\"%\", most_recent=True)\n",
    "    \n",
    "controlled_vocab = ['QC','InjBl','ISTD'] #add _ to beginning. It will be stripped if at begining\n",
    "version_identifier = f\"{username}{analysis_number}\"\n",
    "file_dict = {}\n",
    "groups_dict = {}\n",
    "for f in files:\n",
    "    k = f.name.split('.')[0]\n",
    "    #     get index if any controlled vocab in filename\n",
    "    indices = [i for i, s in enumerate(controlled_vocab) if s.lower() in k.lower()]\n",
    "    prefix = '_'.join(k.split('_')[:11])\n",
    "    if len(indices)>0:\n",
    "        short_name = controlled_vocab[indices[0]].lstrip('_')\n",
    "        group_name = '%s_%s_%s'%(prefix,version_identifier,short_name)\n",
    "        short_name = k.split('_')[9]+'_'+short_name # Prepending POL to short_name\n",
    "    else:\n",
    "        short_name = k.split('_')[12]\n",
    "        group_name = '%s_%s_%s'%(prefix,version_identifier,short_name)\n",
    "        short_name = k.split('_')[9]+'_'+k.split('_')[12]  # Prepending POL to short_name\n",
    "    file_dict[k] = {'file':f,'group':group_name,'short_name':short_name}\n",
    "    groups_dict[group_name] = {'items':[],'name':group_name,'short_name':short_name}\n",
    "df = pd.DataFrame(file_dict).T\n",
    "df.index.name = 'filename'\n",
    "df.reset_index(inplace=True)#['group'].unique()\n",
    "df.drop(columns=['file'],inplace=True)\n",
    "for ug in groups_dict.keys():\n",
    "    for file_key,file_value in file_dict.items():\n",
    "        if file_value['group'] == ug:\n",
    "            groups_dict[ug]['items'].append(file_value['file'])\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: create the groups variable, if the above looks OK\n",
    "\n",
    "groups = []\n",
    "for group_key,group_values in groups_dict.items():\n",
    "    g = metob.Group(name=group_key,items=group_values['items'],short_name=group_values['short_name'])\n",
    "    groups.append(g)        \n",
    "    for item in g.items:\n",
    "        print(g.name,g.short_name,item.name)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 Option A: store the groups variable content in the DB (currently only the long group name is stored)\n",
    "metob.store(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make data frame of short filenames and samplenames\n",
    "Uncomment the below 2 blocks to make short file names and smaple names.<br>\n",
    "This creates a dataframe and a csv file which can be edited, exported and imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make short_filename and short_samplename \n",
    "files = dp.get_metatlas_files(experiment=[f\"{b}%\" for b in run_batches], name=\"%\", most_recent=True)\n",
    "\n",
    "short_filename_delim_ids = [0,2,4,5,7,9,14]\n",
    "short_samplename_delim_ids = [9,12,13,14]\n",
    "short_names_df = pd.DataFrame(columns=['sample_treatment','short_filename','short_samplename'])\n",
    "ctr = 0\n",
    "for f in files:\n",
    "    short_filename = []\n",
    "    short_samplename = []\n",
    "    tokens = f.name.split('.')[0].split('_')\n",
    "    for id in short_filename_delim_ids:\n",
    "        short_filename.append(str(tokens[id]))\n",
    "    for id in short_samplename_delim_ids:\n",
    "        short_samplename.append(str(tokens[id]))\n",
    "    short_filename = \"_\".join(short_filename)\n",
    "    short_samplename = \"_\".join(short_samplename)\n",
    "    short_names_df.loc[ctr, 'full_filename'] = f.name.split('.')[0]\n",
    "    short_names_df.loc[ctr, 'sample_treatment'] = str(tokens[12]) # delim 12\n",
    "    short_names_df.loc[ctr, 'short_filename'] = short_filename\n",
    "    short_names_df.loc[ctr, 'short_samplename'] = short_samplename\n",
    "    short_names_df.loc[ctr, 'last_modified'] = pd.to_datetime(f.last_modified,unit='s')\n",
    "    ctr +=1\n",
    "short_names_df.sort_values(by='last_modified', inplace=True)\n",
    "short_names_df.drop(columns=['last_modified'], inplace=True)\n",
    "short_names_df.drop_duplicates(subset=['full_filename'], keep='last', inplace=True)\n",
    "short_names_df.set_index('full_filename', inplace=True)\n",
    "short_names_df.to_csv(os.path.join(output_dir, 'short_names.csv'), sep=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Select groups of files to operate on\n",
    "\n",
    "Here, you will assign your database groups to a local variable which will be used downstream in the notebook for analyzing your data with an atlas.\n",
    "\n",
    "1. in block below, fill out the fields for name, include_list and exclude_list using text strings from the group names you created in the previous step.  The include/exlcude lists do not need wildcards.  Name is a string unique to all of your groups (ex. fields 0-11 of your filenames)\n",
    "\n",
    "### Typically, you will run one polarity at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups = dp.select_groups_for_analysis(name = f\"{experiment}%\",    # <- edit text search string here\n",
    "                                       most_recent = True,\n",
    "                                       remove_empty = True,\n",
    "                                       include_list = [], exclude_list = ['NEG','QC','InjBL','InjBl'])# ex. ['QC','Blank'])\n",
    "print(\"sorted groups\")\n",
    "groups = sorted(groups, key=lambda x: x.name)\n",
    "for i,a in enumerate(groups):\n",
    "    print(i, a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view metadata about your groups, run the block below\n",
    "metob.to_dataframe(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create new Atlas entries in the Metatlas database from a csv file\n",
    "\n",
    "## QC, IS, and EMA template atlases are available on the google drive.\n",
    "\n",
    "1. Create your atlas as a csv file, check that it looks correct (has all the correct headers and no blank values in rows; all columns are the correct data type\n",
    "2. Save it with the type of atlas (EMA, QC or IS), your initials, the experiment name, the polarity, and the version or timestamp\n",
    "3. Upload it to your nersc project directory (the one you named above).  (If it doesn't work, double check your file permissions are set to at least rw-rw----).\n",
    "4. Run blocks below to create the DB entries for negative and positive mode atlases\n",
    "5. WARNING: Don't run this block over and over again - it will create multiple new DB entries with the same atlas name\n",
    "\n",
    "Required Atlas file headers:\n",
    "\n",
    "rt_min,rt_max,rt_peak,mz,mz_tolerance,adduct,polarity,identification_notes\n",
    "\n",
    "values in rows must be completed for all fields except inchi_key (leaving this blank will not allow you to perform MSMS matching below), and identification notes\n",
    "\n",
    "INFO: store=True will register your atlas in the database.  If you are not sure if your atlas structure is correct, set store=False for the first time your run the block to check if you get an error.  If there is no error, then rerun it with store=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Select Atlas to use\n",
    "## 5a. Load an existing atlas from the database\n",
    "1. The first block will retrieve a list of atlases matching the 'name' string that you enter.  Also, you must enter your username.\n",
    "2. The next block will select one from the list, using the index number.  Make sure to enter the index number for the atlas you want to use for your analysis by setting in this line: my_atlas = atlases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_atlas_file_name is None:\n",
    "    atlases = metob.retrieve('Atlas',name=source_atlas, username=username)\n",
    "    names = []\n",
    "    for i,a in enumerate(atlases):\n",
    "        print(i,a.name,pd.to_datetime(a.last_modified,unit='s'))#len(a.compound_identifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_atlas_file_name is None:\n",
    "    my_atlas = atlases[0]\n",
    "    atlas_df = ma_data.make_atlas_df(my_atlas)\n",
    "    atlas_df['label'] = [cid.name for cid in my_atlas.compound_identifications]\n",
    "    print(my_atlas.name)\n",
    "    metob.to_dataframe([my_atlas])\n",
    "    # the first line of the output will show the dimensions of the atlas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Load atlas from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if csv_atlas_file_name is not None:\n",
    "    my_atlas = dp.make_atlas_from_spreadsheet(csv_atlas_file_name, \n",
    "                                              source_atlas,\n",
    "                                              filetype='csv',\n",
    "                                              sheetname='',\n",
    "                                              polarity = polarity,\n",
    "                                              store=True,\n",
    "                                              mz_tolerance = mz_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: to view your atlas, run this block\n",
    "print(my_atlas.name)\n",
    "atlas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Get EICs and MSMS for all files in your groups, using all compounds in your atlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This block builds the metatlas_dataset variable.  This holds your EIC data (mz, rt, intensity values within your mz and rt ranges).\n",
    "\n",
    "The EIC data contains mz, intensity and RT values across your RT range.  There are two parameters that you will need to edit: extra_time and extra_mz.  Extra time will collect mz, intensity and RT values from outside of your atlas defined min and max rt values.  For example if your rt_min = 1.0, and rt_max = 2.0 and you set extra_time to 0.3, then your new rt range will be 0.7 to 2.3.  This is helpful for checking if you have nearby peaks at the same m/z.  Extra_mz should only be used for troubleshooting.  You should keep this at 0 unless you believe you have poor mass accuracy during your run.  Other ways to address this issue is by changing the mz_tolerance values in your atlas.  Before changing this value, you should check in with a metatlas experienced lab member to discuss when/how to use this value.\n",
    "\n",
    "1. Change the value in \"extra_time = 0.0\" to something like 0.5 to 1.0 for the first EMA runthrough on your files.  This will take longer but collect msms outside your retention windows which allows you to check the msms of nearby peaks before adjusting your rt bounds around the correct peak.\n",
    "2. extra_mz should almost always be set to 0.0   If you need to troubleshoot a low mz compound you could potentially use this value to run it back through with a larger mz error window than what was specified in your atlas (ppm tolerance).\n",
    "\n",
    ">On Your final runthrough, set extra_time to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = []\n",
    "for my_group in groups:\n",
    "    for my_file in my_group.items:\n",
    "        extra_time = 0.5         # NOTE: 0.75 for the first run, 0.5 for final \n",
    "        extra_mz = 0.00\n",
    "        all_files.append((my_file,my_group,atlas_df,my_atlas,extra_time,extra_mz))\n",
    "pool = mp.Pool(processes=min(4, len(all_files)))\n",
    "t0 = time.time()\n",
    "metatlas_dataset = pool.map(ma_data.get_data_for_atlas_df_and_file, all_files)\n",
    "pool.close()\n",
    "pool.terminate()\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data sources tables (atlas_metadata.tab, groups_metadata.tab, groups.tab and [atlasname]_originalatlas.tab within data_sources subfolder)\n",
    "#ma_data.make_data_sources_tables(groups, my_atlas, output_dir) \n",
    "\n",
    "ma_data.make_data_sources_tables(groups, my_atlas, output_dir, polarity=polarity[:3].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6b Optional: Filter atlas for compounds with no or low signals\n",
    "\n",
    "Uncomment the below 3 blocks to filter the atlas.\n",
    "Please ensure that correct polarity is used for the atlases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_df_passing = dp.filter_atlas(atlas_df, metatlas_dataset, num_points, peak_height)\n",
    "print(\"# Compounds in Atlas: \"+str(len(atlas_df)))\n",
    "print(\"# Compounds passing filter: \"+str(len(atlas_df_passing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new atlas and store in database\n",
    "This block creates a filtered atlas with a new name !!\n",
    "Automatically selects this atlas for processing. \n",
    "Make sure to use this atlas for downstream analyses. (NOTE: If you restart kernel or come back to the analysis, you need to reselect this newly created filtered atlas for processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_passing = my_atlas.name+'_filteredby-datapnts'+str(num_points)+'-pkht'+str(peak_height)\n",
    "myAtlas_passing = dp.make_atlas_from_spreadsheet(atlas_df_passing,\n",
    "                          atlas_passing,\n",
    "                          filetype='dataframe',\n",
    "                          sheetname='',\n",
    "                          polarity = polarity,\n",
    "                          store=True,\n",
    "                          mz_tolerance = 12)\n",
    "\n",
    "atlases = dp.get_metatlas_atlas(name=atlas_passing,do_print = True, most_recent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_atlas=atlases[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get MSMS Hits - Skip this if your next step is the RT adjuster GUI.\n",
    "### One of the two blocks below builds the hits variable.  This holds your MSMS spectra (from within your mz, and rt ranges, and within the extra time indicated above).\n",
    "\n",
    "Skip this step if you are about to use the RT adjuster GUI as it will redo the calculation.\n",
    "\n",
    "There are two options for generating the hits variable:\n",
    "1. block A: use when your files have msms. It create the hits variable and also saves a binary (pickled) serialized hits file to the output directory.\n",
    "2. block B: only run if your files were collected in MS1 mode\n",
    "3. If you have already run block A and then the kernel dies, you can skip block A and directly unplickle the binary hits file from the output directory. Skip block A, uncomment the Optional block and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##BLOCK A\n",
    "t0 = time.time()\n",
    "\n",
    "hits=dp.get_msms_hits(metatlas_dataset,extra_time=True,keep_nonmatches=True, frag_mz_tolerance=0.01, ref_loc=msms_refs_file_name)\n",
    "#pickle.dump(hits, open(os.path.join(output_dir,short_polarity+'_hits.pkl'), \"wb\"))\n",
    "\n",
    "print(time.time() - t0)\n",
    "print('%s%s' % (len(hits),' <- total number of MSMS spectra found in your files'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Adjust Retention Times. \n",
    "\n",
    "This block creates an interactive plot.  The top panel displays MSMS from within the two green RT bounds selected below (rt min and max, initially set in atlas).  When the database holds reference spectra, mirror plots are generated with the reference spectra inverted below the sample spectra.  The lower panel displays the EICs overlayed for all of the files in your selected groups.  You can highlight your groups different colors.  It is recommended that you do this, at least, for your extraction blank (or if not available, use a solvent injection blank).  This plot also displays radio buttons that can be interactively selected; the values will be exported in your final identifications table and in your atlas export.  Use these to mark peak/MSMS quality.\n",
    "\n",
    "How to use:\n",
    "1. STEP 1: Set peak flag radio buttons\n",
    "    1. OPTION A (custom flags): fill out the peak flags list (list of strings) \n",
    "        peak_flag_list = ('A','B') some recommendations are below.  \n",
    "    2. OPTION B (default flags): comment out the custom peak_flag_list line.  Uncomment the default peak_flags = \"\". \n",
    "        Flags default to: keep, remove, unresolvable isomers, check.\n",
    "2. STEP 2: Set EIC colors\n",
    "    1. Option A (custom EIC colors): fill out the colorlist in the format of below\n",
    "   > ***\n",
    "   > colorlist = [['color1nameorhexadec','partialgroupstring1'],\n",
    "   >         ['color2nameorhexadec','partialgroupstring2']]\n",
    "   > ***\n",
    "    <ul><li>You can add more comma delimited colors/groups as needed.</li>\n",
    "    <li>These are partial strings that match to you file names (not your group names).</li>\n",
    "    <li>The order they are listed in your list is the order they are displayed in the overlays (first is front, last is back)</li>\n",
    "    <li>Named colors available in matplotlib are here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "          or use hexadecimal values '#000000'</li></ul>\n",
    "    B. Option B (default EIC colors): comment out the custom colorlist lines and uncomment the default colorlist = \"\". \n",
    "        Colors all default to black.\n",
    "3. User the right/left buttons on your keyboard to cycle through compounds in your atlas.\n",
    "4. Use the up/down buttons on your keyboard to cycle through MSMS spectra within the RT bounds of the lower plot.\n",
    "5. Use the horizontal rt min and rt max bars below the plots to adjust the rt bounds around your peak.  If there are multiple peaks, select one at a time and then click up/down to update the msms available in that new RT range.  If necessary evaluate your data in an external program such as mzmine to make sure you are selecting the correct peak.\n",
    "\n",
    "TIPS: use compound_idx = 0 in step 3 to change to a different compound in your atlas using the index number.  If your plot does not fit in your browser window, adjust height and width values.  Use alpha to change the transparency of the lines this is a value 0 (transparent) to 1 (opaque).\n",
    "\n",
    "DO NOT change your RT theoretical peak (the purple line).  It is locked from editing (unless you change a hidden parameter) and only to be changed in special cases.  The measured retention times of your peaks will be calculated and exported in your output files.  These will be compared with the RT theoreticals and used in your evidence of identification table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = AnalysisIdentifiers(project_directory, experiment, output_type, polarity, analysis_number, google_folder,\n",
    "          source_atlas=my_atlas.name,\n",
    "          copy_atlas=False,\n",
    "          username=username,\n",
    "          exclude_files=None,\n",
    "          include_groups=None,\n",
    "          exclude_groups=None,\n",
    "          groups_controlled_vocab=None,\n",
    "          lcmsruns=files,\n",
    "          all_groups=groups)\n",
    "if clear_cache:\n",
    "    shutil.rmtree(ids.cache_dir)\n",
    "metatlas_dataset = mads.MetatlasDataset(ids=ids, msms_refs_loc=msms_refs_file_name, max_cpus=max_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agui = metatlas_dataset.annotation_gui(compound_idx=0, width=15, height=3, colors=line_colors, adjustable_rt_peak=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive output generation and RClone upload\n",
    "The next block generates all outputs and uploads then to Google Drive. If you use the next block, you can skip blocks 8 and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mads.post_annotation(metatlas_dataset, require_all_evaluated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create filtered atlas excluding compounds marked removed\n",
    "\n",
    "Re-run the following before filtering atlas\n",
    "1. Get Groups (include InjBl)\n",
    "2. Get Atlas\n",
    "3. Get Data\n",
    "4. Get MSMS Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(atlas_all, atlas_kept, atlas_removed) = dp.filter_by_remove(atlas_df, metatlas_dataset)\n",
    "#print(\"# Compounds Total: \"+str(len(atlas_all)))\n",
    "#print(\"# Compounds Kept: \"+str(len(atlas_kept)))\n",
    "#print(\"# Compounds Removed: \"+str(len(atlas_removed)))\n",
    "\n",
    "(atlas_kept, atlas_removed) = dp.filter_by_remove(atlas_df, metatlas_dataset)\n",
    "print(\"# Compounds Kept: \"+str(len(atlas_kept)))\n",
    "print(\"# Compounds Removed: \"+str(len(atlas_removed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE SURE YOU PUT IN THE CORRECT POLARITY!!!!\n",
    "\n",
    "atlasfilename=my_atlas.name+'_kept'  # <- enter the name of the atlas to be stored\n",
    "\n",
    "names = dp.make_atlas_from_spreadsheet(atlas_kept, \n",
    "                                       atlasfilename,  # <- DO NOT EDIT THIS LINE\n",
    "                                       filetype='dataframe',\n",
    "                                       sheetname='',\n",
    "                                       polarity = polarity,\n",
    "                                       store=True,\n",
    "                                       mz_tolerance = 7\n",
    "                                      )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the following after filtering atlas\n",
    "1. Restart kernel\n",
    "2. Get Groups\n",
    "3. Get Atlas (look for the *_kept atlas)\n",
    "4. Get Data\n",
    "5. Get MSMS Hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Export results files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Atlas to a Spreadsheet\n",
    "\n",
    "The peak flags that you set and selected from the rt adjuster radio buttons will be saved in a column called id_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_identifications = dp.export_atlas_to_spreadsheet(my_atlas,os.path.join(output_dir,'%s_%s%s.csv' % (polarity,my_atlas.name,\"export\")))\n",
    "print(my_atlas.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS match scores, stats sheets, and final identification table\n",
    "\n",
    "This block creates a number of files:\n",
    "\n",
    "1. compound_scores.csv\n",
    "2. stats_table.tab\n",
    "3. filtered and unfiltered peak heights, areas, msms scores, mz centroid, mz ppm error, num of fragment matches, rt delta, rt peak\n",
    "4. final identification sheet that is formatted for use as a supplemental table for manuscript submission.  You will need to manually complete some columns.  Please discuss with Ben, Katherine, Daniel or Suzie before using for the first time.\n",
    "\n",
    "THe kwargs below will set the filtering points for the parameters indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'min_intensity': 1e4,   # strict = 1e5, loose = 1e3\n",
    "          'rt_tolerance': .5,    #>= shift of median RT across all files for given compound to reference\n",
    "          'mz_tolerance': 20,      # strict = 5, loose = 25; >= ppm of median mz across all files for given compound relative to reference\n",
    "          'min_msms_score': .6, 'allow_no_msms': True,     # strict = 0.6, loose = 0.3 <= highest compound dot-product score across all files for given compound relative to reference\n",
    "          'min_num_frag_matches': 1, 'min_relative_frag_intensity': .001}   # strict = 3 and 0.1, loose = 1, 0.01 number of matching mzs when calculating max_msms_score and ratio of second highest to first highest intensity of matching sample mzs\n",
    "scores_df = fa.make_scores_df(metatlas_dataset,hits)\n",
    "scores_df['passing'] = fa.test_scores_df(scores_df, **kwargs)\n",
    "\n",
    "pass_atlas_df, fail_atlas_df, pass_dataset, fail_dataset = fa.filter_atlas_and_dataset(scores_df, atlas_df, metatlas_dataset, column='passing')\n",
    "\n",
    "fa.make_stats_table(input_dataset = metatlas_dataset, msms_hits = hits, output_loc = output_dir,min_peak_height=1e5,use_labels=True,min_msms_score=0.01,min_num_frag_matches=1,include_lcmsruns = [],exclude_lcmsruns = ['QC'], polarity=polarity)\n",
    "os.makedirs(os.path.join(output_dir,'stats_tables'), exist_ok=True)\n",
    "scores_df.to_csv(os.path.join(output_dir,'stats_tables',polarity+'_compound_scores.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export EIC chromatograms as individual pdfs for each compound\n",
    "\n",
    "1.  There are three options for formatting your EIC output using the \"group =\" line below:\n",
    "    1. 'page' will print each sample group on a new page of a pdf file\n",
    "    2. 'index' will label each group with a letter\n",
    "    3. None will print all of the groups on one page with very small subplot labels\n",
    "2. The Y axis scale can be shared across all files using share_y = True or set to the max within each file using share_y = False\n",
    "3. To use short names for plots, short_names_df should be provided as input. Additionally the header column to be used for short names should be provided as follows (short_names_df=short_names_df, short_names_header='short_samplename'). Header options are sample_treatment, short_filename, short_samplename. These are optional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group = 'index' # 'page' or 'index' or None\n",
    "save = True\n",
    "share_y = True\n",
    "\n",
    "dp.make_chromatograms(input_dataset=metatlas_dataset, include_lcmsruns = [],exclude_lcmsruns = ['InjBl','InjBL','QC','Blank','blank'], group=group, share_y=share_y, save=save, output_loc=output_dir, short_names_df=short_names_df, short_names_header='short_samplename', polarity=polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export MSMS mirror plots as individual pdfs for each compound\n",
    "\n",
    "1. use_labels = True will use the compound names you provided in your atlas, if you set it to false, the compounds will be named with the first synonym available from pubchem which could be a common name, iupac name, cas number, vendor part number, etc. \n",
    "2.  The include and exclude lists will match partial strings in filenames, do not use wildcards.\n",
    "3. If short_names_df is provided as input, short_samplename is used for plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dp.make_identification_figure_v2(input_dataset = metatlas_dataset, msms_hits=hits, use_labels=True, include_lcmsruns = [],exclude_lcmsruns = ['InjBl','InjBL','QC','Blank','blank','ExCtrl'], output_loc=output_dir,  short_names_df=short_names_df, polarity=polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sheets\n",
    "1. To include short names in the output, short_names_df should be provided as input to make_output_dataframe. \n",
    "2. ylabel is optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_height = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='peak_height', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "peak_area = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='peak_area', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "mz_peak = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='mz_peak', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "rt_peak = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [],fieldname='rt_peak', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "mz_centroid = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='mz_centroid', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)\n",
    "rt_centroid = dp.make_output_dataframe(input_dataset = metatlas_dataset,include_lcmsruns = [],exclude_lcmsruns = [], fieldname='rt_centroid', output_loc=os.path.join(output_dir,polarity+'_data_sheets'), short_names_df=short_names_df, polarity=polarity, use_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.make_boxplot_plots(rt_peak, output_loc=os.path.join(output_dir, polarity+'_boxplot_rt_peak'), ylabel=\"RT Peak\")\n",
    "dp.make_boxplot_plots(peak_height, output_loc=os.path.join(output_dir, polarity+'_boxplot_peak_height'), ylabel=\"Peak Height\")\n",
    "dp.make_boxplot_plots(mz_centroid, output_loc=os.path.join(output_dir, polarity+'_boxplot_mz_centroid'), ylabel=\"MZ Centroid\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Metatlas Targeted",
   "language": "python",
   "name": "metatlas-targeted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
