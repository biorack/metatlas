{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=invalid-name,missing-module-docstring\n",
    "\n",
    "# A list of experiment IDs to pull LCMS runs from\n",
    "# if None, then the value of the \"experiment\" variable will be used\n",
    "run_batches = None\n",
    "\n",
    "# The name of a workflow defined in the configuration file\n",
    "workflow_name = None\n",
    "\n",
    "# The name of an analysis within the workflow\n",
    "analysis_name = None\n",
    "\n",
    "# source atlas' \"unique_id\" field in the database\n",
    "source_atlas_unique_id = None\n",
    "\n",
    "# if copy_atlas is True, then generate an atlas specifically for this analysis_number\n",
    "# should only be set to False if an analysis will not be modifying the atlas or RT ranges\n",
    "copy_atlas = None\n",
    "\n",
    "# one of 'positive' or 'negative'\n",
    "polarity = None\n",
    "\n",
    "# an integer, increment if you need to redo your analysis\n",
    "# will be appended to your username to create analysis_id\n",
    "analysis_number = None\n",
    "\n",
    "# experiment ID that must match the parent folder containing the LCMS output files\n",
    "# An example experiment ID is '20201116_JGI-AK_LH_506489_SoilWarm_final_QE-HF_HILICZ_USHXG01530'\n",
    "experiment = None\n",
    "\n",
    "# list of substrings that will group together when creating groups\n",
    "# this provides additional grouping beyond the default grouping on field #12\n",
    "groups_controlled_vocab = None\n",
    "\n",
    "# The following include/exclude groups/lcmsruns dictionaries are used to define\n",
    "# the filtering of groups or lcmsruns at different steps of the analysis\n",
    "# pipeline.\n",
    "#\n",
    "# Each of the values within the dictionary should either be None or a list of\n",
    "# strings. If a None value is supplied below, then the value will be loaded\n",
    "# from the configuration file (config_file_name). If a list value is provided\n",
    "# below, then the list will be utilized (overriding the value in the\n",
    "# configuration file).\n",
    "#\n",
    "# The value associated with the 'always' key will be appeneded to each of the\n",
    "# other values within the same dictionary. This appending occurs after all\n",
    "# dictionary values given in this notebook have been merged with the values\n",
    "# given in the configuration file.\n",
    "#\n",
    "# If the configuration file does not define a key-value pair and the below\n",
    "# dictionary has None, then no filtering will be performed.\n",
    "\n",
    "# group will only be used if their name has a substring match to this list of strings\n",
    "include_groups = dict(  # noqa: C408\n",
    "    always=None,\n",
    "    gui=None,\n",
    "    qc_outputs=None,\n",
    "    ids_spreadsheet=None,\n",
    "    chromatograms=None,\n",
    "    data_sheets=None,\n",
    "    box_plots=None,\n",
    ")\n",
    "\n",
    "# Exclude groups with names containing any of the substrings in this list.\n",
    "# Generally you will want to include polarities you are not using\n",
    "# such as ['NEG', 'FPS'] for a positive polarity analysis.\n",
    "exclude_groups = dict(  # noqa: C408\n",
    "    always=None,\n",
    "    gui=None,\n",
    "    qc_outputs=None,\n",
    "    ids_spreadsheet=None,\n",
    "    chromatograms=None,\n",
    "    data_sheets=None,\n",
    "    box_plots=None,\n",
    ")\n",
    "\n",
    "# LCMS runs will only be used if their name contain one or more of the\n",
    "# substrings in the corresponding list.\n",
    "include_lcmsruns = dict(  # noqa: C408\n",
    "    always=None,\n",
    "    gui=None,\n",
    "    qc_outputs=None,\n",
    "    ids_spreadsheet=None,\n",
    "    chromatograms=None,\n",
    "    data_sheets=None,\n",
    "    box_plots=None,\n",
    ")\n",
    "\n",
    "# LCMS runs will removed if their name contain one or more of the\n",
    "# substrings in the corresponding list.\n",
    "exclude_lcmsruns = dict(  # noqa: C408\n",
    "    always=None,\n",
    "    gui=None,\n",
    "    qc_outputs=None,\n",
    "    ids_spreadsheet=None,\n",
    "    chromatograms=None,\n",
    "    data_sheets=None,\n",
    "    box_plots=None,\n",
    ")\n",
    "\n",
    "# Create outputs used to QC the run\n",
    "generate_qc_outputs = None\n",
    "\n",
    "# thresholds for filtering out compounds with weak MS1 signals\n",
    "# set to None to disable a filter\n",
    "num_points = None\n",
    "peak_height = None\n",
    "\n",
    "# if True, the post_annotation() function will remove atlas rows marked\n",
    "# 'Remove' before generating output files\n",
    "filter_removed = None\n",
    "\n",
    "# list of tuples contain string with color name and substring pattern.\n",
    "# Lines in the EIC plot will be colored by the first substring pattern\n",
    "# that has a match within the name of the hdf5_file. The order they are\n",
    "# listed in your list is the order they are displayed in the overlays\n",
    "# (first is front, last is back). Named colors available in matplotlib\n",
    "# are here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html\n",
    "# or use hexadecimal values '#000000'. Lines default to black.\n",
    "line_colors = None\n",
    "\n",
    "# Set to False to disable check that all compounds have either been\n",
    "# removed or rated within the annotation GUI before generating outputs.\n",
    "require_all_evaluated = None\n",
    "\n",
    "# If True, then create the main set of outputs\n",
    "generate_analysis_outputs = None\n",
    "\n",
    "# Groups to be excluded when generating the post annotation outputs:\n",
    "exclude_groups_for_analysis_outputs = None\n",
    "\n",
    "# include MSMS fragment ions in the output documents?\n",
    "# has no effect if generate_post_annotation_outputs is False\n",
    "export_msms_fragment_ions = None\n",
    "\n",
    "# Setting this to True will remove the cache of MSMS hits\n",
    "# if you don't see MSMS data for any of your compounds in RT adjuster GUI,\n",
    "# then you might want to try settings this to True. However, it will\n",
    "# make your notebook take significantly longer to run.\n",
    "# The cache is per experiment, so clearing the cache will impact other\n",
    "# notebooks for this same experiment.\n",
    "clear_cache = None\n",
    "\n",
    "# This value will always be automatically passed in from the RT_Alignment\n",
    "# notebook and you should not manually set this parameter.\n",
    "rt_alignment_number = None\n",
    "\n",
    "# The rest of this block contains project independent parameters\n",
    "\n",
    "# Configuration file location\n",
    "config_file_name = None\n",
    "\n",
    "# to use an older version of the metatlas source code, set this to a commit id,\n",
    "# branch name, or tag. If None, then use the the \"main\" branch.\n",
    "source_code_version_id = None\n",
    "\n",
    "# Full path to the directory where you want this notebook to store data.\n",
    "# A subdirectory will be auto created within this directory for each project.\n",
    "# You can place this anywhere on cori's filesystem, but placing it within your\n",
    "# global home directory is recommended so that you do not need to worry about\n",
    "# your data being purged. Each project will take on the order of 100 MB.\n",
    "project_directory = None\n",
    "\n",
    "# ID from Google Drive URL for base output folder .\n",
    "# The default value is the ID that corresponds to 'JGI_Metabolomics_Projects'.\n",
    "google_folder = None\n",
    "\n",
    "# maximum number of CPUs to use\n",
    "# when running on jupyter.nersc.gov, you are not allowed to set this above 4\n",
    "max_cpus = None\n",
    "\n",
    "# Threshold for how much status information metatlas functions print in the notebook\n",
    "# levels are 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'\n",
    "log_level = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=wrong-import-position,import-error,missing-class-docstring\n",
    "import logging  # noqa: E402\n",
    "from pathlib import Path  # noqa: E402\n",
    "\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "parameters = {k: v for k, v in globals().items() if k[0] != \"_\" and k not in [\"In\", \"Out\", \"get_ipython\", \"exit\", \"quit\", \"open\"]}\n",
    "logger = logging.getLogger(\"metatlas.jupyter\")\n",
    "kernel_def = \"\"\"{\"argv\":[\"shifter\",\"--entrypoint\",\"--image=doejgi/metatlas_shifter:latest\",\"/usr/local/bin/python\",\"-m\",\n",
    "                 \"ipykernel_launcher\",\"-f\",\"{connection_file}\"],\"display_name\": \"Metatlas Targeted\",\"language\": \"python\",\n",
    "                 \"metadata\": { \"debugger\": true }}\"\"\"\n",
    "kernel_file_name = Path.home() / \".local\" / \"share\" / \"jupyter\" / \"kernels\" / \"metatlas-targeted\" / \"kernel.json\"\n",
    "try:\n",
    "    has_root_kernel = Path(\"/root/.local/share/jupyter/kernels/papermill/kernel.json\").is_file()\n",
    "except PermissionError:\n",
    "    has_root_kernel = False\n",
    "if not has_root_kernel and not kernel_file_name.is_file():\n",
    "    kernel_file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with kernel_file_name.open(mode=\"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(kernel_def)\n",
    "    logger.critical('CRITICAL: Notebook kernel has been installed. Set kernel to \"Metatlas Targeted\" and re-run notebook.')\n",
    "    raise StopExecution\n",
    "try:\n",
    "    from metatlas.tools import notebook, config  # noqa: E402\n",
    "except ImportError as err:\n",
    "    logger.critical('CRITICAL: Set notebook kernel to \"Metatlas Targeted\" and re-run notebook.')\n",
    "    raise StopExecution from err\n",
    "configuration, workflow, analysis = config.get_config(parameters)\n",
    "notebook.setup(analysis.parameters.log_level, analysis.parameters.source_code_version_id)\n",
    "\n",
    "import getpass  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "from IPython.display import display, HTML  # noqa: E402\n",
    "from metatlas.plots import dill2plots as dp  # noqa: E402\n",
    "from metatlas.datastructures import metatlas_objects as metob  # noqa: E402\n",
    "from metatlas.datastructures.analysis_identifiers import AnalysisIdentifiers  # noqa: E402\n",
    "from metatlas.targeted.process import pre_annotation, annotation_gui, post_annotation  # noqa: E402\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option(\"display.max_rows\", 5000)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "assert experiment is not None\n",
    "assert source_atlas_unique_id is not None\n",
    "assert project_directory is not None\n",
    "run_batches = run_batches if run_batches is not None else [experiment]\n",
    "%matplotlib widget\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Set output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_ids = AnalysisIdentifiers(\n",
    "    project_directory=project_directory,\n",
    "    experiment=experiment,\n",
    "    configuration=configuration,\n",
    "    workflow=workflow_name,\n",
    "    analysis=analysis_name,\n",
    "    analysis_number=analysis_number,\n",
    "    source_atlas_unique_id=source_atlas_unique_id,\n",
    "    username=None,\n",
    "    lcmsruns=None,\n",
    "    all_groups=None,\n",
    ")\n",
    "output_dir = initial_ids.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Groups\n",
    "## Find your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = dp.get_metatlas_files(experiment=run_batches, name=\"%\", most_recent=True)\n",
    "df = metob.to_dataframe(files)\n",
    "display(df[[\"experiment\", \"name\", \"username\", \"acquisition_time\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTION A: Automated Group Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: View the groups\n",
    "\n",
    "initial_ids.set_output_state(analysis.parameters, \"gui\")\n",
    "\n",
    "files = dp.get_metatlas_files(experiment=run_batches, name=\"%\", most_recent=True)\n",
    "\n",
    "controlled_vocab = [\"QC\", \"InjBl\", \"ISTD\"]  # add _ to beginning. It will be stripped if at begining\n",
    "version_identifier = f\"{username}_0_{analysis_number}\"\n",
    "file_dict = {}\n",
    "groups_dict = {}\n",
    "# WARNING - This currently does not filter based on initial_ids.include_lcmsruns\n",
    "for f in files:\n",
    "    if any(exclude_string in f.name for exclude_string in initial_ids.exclude_lcmsruns):\n",
    "        continue\n",
    "    k = f.name.split(\".\")[0]\n",
    "    #     get index if any controlled vocab in filename\n",
    "    indices = [i for i, s in enumerate(controlled_vocab) if s.lower() in k.lower()]\n",
    "    prefix = \"_\".join(k.split(\"_\")[:11])\n",
    "    if len(indices) > 0:\n",
    "        short_name = controlled_vocab[indices[0]].lstrip(\"_\")\n",
    "        group_name = f\"{prefix}_{version_identifier}_{short_name}\"\n",
    "        short_name = k.split(\"_\")[9] + \"_\" + short_name  # Prepending POL to short_name\n",
    "    else:\n",
    "        short_name = k.split(\"_\")[12]\n",
    "        group_name = f\"{prefix}_{version_identifier}_{short_name}\"\n",
    "        short_name = k.split(\"_\")[9] + \"_\" + k.split(\"_\")[12]  # Prepending POL to short_name\n",
    "    file_dict[k] = {\"file\": f, \"group\": group_name, \"short_name\": short_name}\n",
    "    groups_dict[group_name] = {\"items\": [], \"name\": group_name, \"short_name\": short_name}\n",
    "df = pd.DataFrame(file_dict).T\n",
    "df.index.name = \"filename\"\n",
    "df.reset_index(inplace=True)  # ['group'].unique()\n",
    "df.drop(columns=[\"file\"], inplace=True)\n",
    "for name, data in groups_dict.items():\n",
    "    for file_value in file_dict.values():\n",
    "        if file_value[\"group\"] == name:\n",
    "            data[\"items\"].append(file_value[\"file\"])\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: create the groups variable, if the above looks OK\n",
    "\n",
    "groups = []\n",
    "for group_key, group_values in groups_dict.items():\n",
    "    g = metob.Group(name=group_key, items=group_values[\"items\"], short_name=group_values[\"short_name\"])\n",
    "    groups.append(g)\n",
    "    for item in g.items:\n",
    "        print(g.name, g.short_name, item.name)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 Option A: store the groups variable content in the DB (currently only the long group name is stored)\n",
    "metob.store(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Select groups of files to operate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups = dp.select_groups_for_analysis(\n",
    "    name=f\"{experiment}%\",  # <- edit text search string here\n",
    "    most_recent=True,\n",
    "    remove_empty=True,\n",
    "    include_list=initial_ids.include_groups,\n",
    "    exclude_list=initial_ids.exclude_groups,\n",
    ")\n",
    "print(\"sorted groups\")\n",
    "groups = sorted(groups, key=lambda x: x.name)\n",
    "for i, a in enumerate(groups):\n",
    "    print(i, a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to view metadata about your groups, run the block below\n",
    "metob.to_dataframe(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load data, generating metatlas_dataset\n",
    "\n",
    "Reads data from .h5 files and generates a metatlas_dataset instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metatlas_dataset = pre_annotation(\n",
    "    experiment=experiment,\n",
    "    rt_alignment_number=rt_alignment_number,\n",
    "    analysis_number=analysis_number,\n",
    "    source_atlas_unique_id=source_atlas_unique_id,\n",
    "    configuration=configuration,\n",
    "    workflow=workflow,\n",
    "    analysis=analysis,\n",
    "    lcmsruns=files,\n",
    "    all_groups=groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Annotation GUI\n",
    "If you are re-running this notebook and do not need to make additional changes to RT min/max bounds, then you can skip running the next code cell. Skipping will save you from calculating MSMS hits twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agui = annotation_gui(data=metatlas_dataset, compound_idx=0, width=15, height=3, colors=analysis.parameters.line_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generate standard outputs and upload to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_annotation(data=metatlas_dataset, configuration=configuration, workflow=workflow, analysis=analysis)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Metatlas Targeted",
   "language": "python",
   "name": "metatlas-targeted"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
