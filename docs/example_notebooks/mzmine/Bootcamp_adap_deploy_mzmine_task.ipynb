{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import requests\n",
    "# from requests.auth import HTTPBasicAuth\n",
    "# import sys\n",
    "# if sys.version_info[0] < 3: \n",
    "#     from StringIO import StringIO\n",
    "# else:\n",
    "#     from io import StringIO\n",
    "    \n",
    "# url = 'https://metabolomics.app.labkey.host/_webdav/Metabolite%20Atlas/%40files/Probe_peak_height.tab?contentDisposition=attachment'\n",
    "# r = requests.get(url, auth=HTTPBasicAuth(u,p))\n",
    "# df = pd.read_csv(StringIO(r.text),sep='\\t')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NERSC=', True)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "import sys, os\n",
    "import glob\n",
    "import json\n",
    "import copy\n",
    "from itertools import product\n",
    "\n",
    "sys.path.insert(0,'/global/homes/b/bpb/repos/metatlas')\n",
    "\n",
    "# # sys.path.insert(1,'/global/projecta/projectdirs/metatlas/anaconda/lib/python2.7/site-packages' )\n",
    "from ast import literal_eval\n",
    "import metatlas.metatlas_objects as metob\n",
    "# from metatlas.helpers import mzmine_helpers as mzm\n",
    "from metatlas.helpers import mzmine_batch_tools_adap as mzm\n",
    "from metatlas.helpers import dill2plots as dp\n",
    "from metatlas.helpers import metatlas_get_data_helper_fun as ma_data\n",
    "from metatlas.helpers import chromatograms_mp_plots as cp\n",
    "\n",
    "import collections\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# import multiprocessing as mp\n",
    "\n",
    "from distutils.util import strtobool\n",
    "from datetime import datetime\n",
    "from shutil import copyfile\n",
    "\n",
    "from ast import literal_eval\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option(\"display.max_colwidth\", 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# paths = ['/global/cscratch1/sd/bpb/mzmine_jobs/20171214_KBL_JL_C18_Poplar_FungInnoc_positive_8C552483/*height.csv',\n",
    "#          '/global/cscratch1/sd/bpb/mzmine_jobs/20171214_KBL_JL_C18_Poplar_FungInnoc_negative_8302CCEF/*height.csv']\n",
    "\n",
    "# mz = [229.981110,227.966560]\n",
    "# rt = [4.77,4.77]\n",
    "# polarity = ['positive','negative']\n",
    "# rt_range = [0.2,0.2]\n",
    "# mz_range = [0.01,0.01]\n",
    "\n",
    "# fig,ax = plt.subplots(nrows=2,ncols=1,figsize=(12,8),sharex=True)\n",
    "# ax = ax.flatten()\n",
    "# df_summary = []\n",
    "# for i,p in enumerate(paths):\n",
    "#     my_file = glob.glob(p)[-1]\n",
    "#     df = pd.read_csv(my_file)\n",
    "#     print(df.shape[0])\n",
    "#     pk_cols = [c for c in df.columns if 'Peak height' in c]\n",
    "#     idx_mz = abs(df['row m/z'] - mz[i])<0.01\n",
    "#     idx_rt = abs(df['row retention time'] - rt[i])<0.2\n",
    "#     idx_mzrt = idx_mz & idx_rt\n",
    "#     print(sum(idx_mz),sum(idx_rt),sum(idx_mzrt))\n",
    "#     s = df.loc[idx_mzrt,pk_cols]\n",
    "    \n",
    "#     temp = pd.merge(pd.DataFrame(s.median(axis=1),columns=['sum intensity']),\n",
    "#                    df.loc[idx_mzrt,['row m/z','row retention time']],\n",
    "#                    left_index=True,right_index=True)\n",
    "    \n",
    "#     x = np.asarray([int(vv[0].replace('_Run','').replace('.','')) if len(vv)>0 else 0 for vv in [re.findall(\"_Run\\d+\\.\",v) for v in s.columns]])\n",
    "#     y = s.values.flatten()\n",
    "    \n",
    "\n",
    "    \n",
    "#     idx = np.argsort(x).flatten()\n",
    "#     x = x[idx]\n",
    "#     y = y[idx]\n",
    "#     ax[i].plot(x,y,'.')\n",
    "#     if i==1:\n",
    "#         ax[i].set_xlabel('Run Order Index')\n",
    "        \n",
    "#     ax[i].set_ylabel('%s mode\\nABMBA peak height (au)'%polarity[i])\n",
    "    \n",
    "#     temp['polarity'] = polarity[i]\n",
    "#     temp['rt_range'] = rt_range[i]\n",
    "#     temp['mz_range'] = mz_range[i]\n",
    "#     df_summary.append(temp)\n",
    "\n",
    "# pd.concat(df_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = ['/global/cscratch1/sd/bpb/mzmine_jobs/20171214_KBL_JL_C18_Poplar_FungInnoc_positive_8C552483/*height.csv',\n",
    "#          '/global/cscratch1/sd/bpb/mzmine_jobs/20171214_KBL_JL_C18_Poplar_FungInnoc_negative_8302CCEF/*height.csv']\n",
    "\n",
    "# mz = [218.12812,216.11357]\n",
    "# rt = [2.34,2.34]\n",
    "# polarity = ['positive','negative']\n",
    "# rt_range = [0.2,0.2]\n",
    "# mz_range = [0.01,0.01]\n",
    "\n",
    "# fig,ax = plt.subplots(nrows=1,ncols=2)\n",
    "# ax = ax.flatten()\n",
    "# df_summary = []\n",
    "# for i,p in enumerate(paths):\n",
    "#     my_file = glob.glob(p)[-1]\n",
    "#     df = pd.read_csv(my_file)\n",
    "#     print(df.shape[0])\n",
    "#     pk_cols = [c for c in df.columns if 'Peak height' in c]\n",
    "#     idx_mz = abs(df['row m/z'] - mz[i])<0.01\n",
    "#     idx_rt = abs(df['row retention time'] - rt[i])<0.2\n",
    "#     idx_mzrt = idx_mz & idx_rt\n",
    "#     print(sum(idx_mz),sum(idx_rt),sum(idx_mzrt))\n",
    "#     s = df.loc[idx_mzrt,pk_cols].median(axis=1)\n",
    "    \n",
    "#     temp = pd.merge(pd.DataFrame(s,columns=['sum intensity']),\n",
    "#                    df.loc[idx_mzrt,['row m/z','row retention time']],\n",
    "#                    left_index=True,right_index=True)\n",
    "#     temp['polarity'] = polarity[i]\n",
    "#     temp['rt_range'] = rt_range[i]\n",
    "#     temp['mz_range'] = mz_range[i]\n",
    "#     df_summary.append(temp)\n",
    "\n",
    "# pd.concat(df_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outline of operational steps\n",
    "\n",
    "Given a new batch file, log it at a workflow\n",
    "\n",
    "For a workflow perform and evaluate parameter scanning\n",
    "\n",
    "\n",
    "For a workflow setup ability to run that job on datasets\n",
    "\n",
    "1. Make a new parameter template from a batch file\n",
    "\n",
    "2. Use a parameter template to search for peaks\n",
    "\n",
    "3. Scan parameters within a range for a template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a new csv parameter entry from a batch file.  You do this and then put the parameters into a new google sheet with a column called param_id.\n",
    "\n",
    "An example can be seen here:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/17bf-GmYfpYKyqW_attFLli3_Glva2cnCuHlFP6FMP-Y/edit#gid=0\n",
    "\n",
    "Role this into the LIMS once the LIMS is stablized.\n",
    "\n",
    "You shouldn't have to do this very often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mzm = reload(mzm)\n",
    "\n",
    "# # xml_file = '/project/projectdirs/metatlas/projects/mzmine_parameters/batch_files/bootcamp_template_batch_file.xml'\n",
    "# # xml_file = '/global/homes/b/bpb/Downloads/batch_louie_nomsms_noisotope.xml'\n",
    "# # xml_file = '/global/homes/b/bpb/Downloads/batch_louie_withmsms_withisotope.xml'\n",
    "# xml_file = '/global/homes/b/bpb/Downloads/batch_template_2p39.xml'\n",
    "\n",
    "# csv_file = '/global/homes/b/bpb/Downloads/mzmine_workflow.csv'\n",
    "# df=mzm.mzmine_xml_to_csv(xml_file=xml_file,csv_file=csv_file)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mzm = reload(mzm)\n",
    "\n",
    "# xml_file = '/global/homes/b/bpb/Downloads/1904_MZMine_2_39_QE_RP_UCSD_JGI_sens_deconv_basicadduct_v4dupl.xml'\n",
    "# csv_file = '/global/homes/b/bpb/Downloads/mzmine_workflow.csv'\n",
    "# df=mzm.mzmine_xml_to_csv(xml_file=xml_file,csv_file=csv_file,pop_input_files=False)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn spreadsheet of specified jobs into job entry.\n",
    "\n",
    "Each job entry will tell you a specific google sheet with parameter entries in it.  An example is here:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1nQCz3Qpr_lmiz6MZJznaBFlvujySeFW5EG-C3mGPSDE/edit#gid=2011516151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "dp = reload(dp)\n",
    "df = dp.get_google_sheet(notebook_name='mzmine_jobs_20190625',sheet_name='JOBS',literal_cols=['file_filters','groups'])\n",
    "df = df[df['done']==False]\n",
    "\n",
    "mzmine_things = df.T.to_dict().values()\n",
    "print(len(mzmine_things))\n",
    "# print(mzmine_things[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see all the unique types of mzmine parameters that are needed and fetch them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 unique parameter sheets\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "for m in mzmine_things:\n",
    "    temp.append(m['parameter_sheet'])\n",
    "unique_parameter_sheets_tablenames = pd.unique(temp)\n",
    "\n",
    "print('There are %d unique parameter sheets'%len(unique_parameter_sheets_tablenames))\n",
    "unique_parameter_sheets = {}\n",
    "for s in unique_parameter_sheets_tablenames:\n",
    "    df = dp.get_google_sheet(notebook_name=s,sheet_name='Sheet1')\n",
    "    new_cols = []\n",
    "    for c in df.columns:\n",
    "        if c.startswith('('):\n",
    "            new_cols.append(literal_eval(c))\n",
    "        else:\n",
    "            new_cols.append(c)\n",
    "    df.columns=new_cols\n",
    "    unique_parameter_sheets[s] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for each mzmine thing, get its parameter set and turn it into a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/global/homes/b/bpb/gnps_password.txt','r') as fid:\n",
    "    gnps_password = fid.read().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    df = unique_parameter_sheets[m['parameter_sheet']]\n",
    "    d = df[df['param_id']==m['parameter_id']].to_dict(orient='records')[-1]\n",
    "    d.pop('param_id', None)\n",
    "    d = collections.OrderedDict(d)\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    #replace the polarity in the crop filter module\n",
    "    for k,v in d.items():\n",
    "        if 'polarity' in k:\n",
    "            d[k] = m['polarity'].upper()\n",
    "    \n",
    "    #rename all the output files\n",
    "    for k,v in d.items():\n",
    "        try:\n",
    "            if 'placeholder_filename' in v:\n",
    "                if 'gnps-job' in v:\n",
    "                    d[k] = v.replace('placeholder_filename',m['unique_basename'])\n",
    "                else:\n",
    "                    d[k] = v.replace('placeholder_filename',new_basename)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "#     #replace gnps password\n",
    "#     for k,v in d.items():\n",
    "#         try:\n",
    "#             if 'justapassword' in v:\n",
    "#                 d[k] = v.replace('justapassword',gnps_password)\n",
    "#         except TypeError:\n",
    "#             pass\n",
    "    \n",
    "    mzmine_things[i]['param_dict_flat'] = d\n",
    "    \n",
    "    #This is a good place to make the values strings.  The xml maker needs strings later on so might as well do it here\n",
    "    str_d = {}\n",
    "    for k,v in d.items():\n",
    "        str_d[k] = str(v)\n",
    "        \n",
    "    #unflatten it\n",
    "    mzmine_things[i]['param_dict_unflat'] = mzm.unflatten(str_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/pymysql/cursors.py:166: Warning: (1287, \"'@@tx_isolation' is deprecated and will be removed in a future release. Please use '@@transaction_isolation' instead\")\n",
      "  result = self._query(query)\n"
     ]
    }
   ],
   "source": [
    "mzm = reload(mzm)\n",
    "for i,m in enumerate(mzmine_things):\n",
    "    mzmine_things[i]['file_list'] = mzm.get_files(m['groups'],m['filename_substring'],m['file_filters'],m['is_group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move all the files to cscratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsync -zavh /project/projectdirs/metatlas/raw_data/ /global/cscratch1/sd/bpb/raw_data\n",
    "# rsync -zavh /project/projectdirs/metatlas/projects/jgi_projects/ /global/cscratch1/sd/bpb/mzmine_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    # add on raw data files\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    new_save_step = {'@method': \"net.sf.mzmine.modules.projectmethods.projectsave.ProjectSaveAsModule\",\n",
    "                     'parameter': {'@name':\"Project file\",'current_file':'%s_workspace.mzmine'%new_basename}}\n",
    "        \n",
    "    new_raw_data = {'@method': 'net.sf.mzmine.modules.rawdatamethods.rawdataimport.RawDataImportModule',\n",
    "                    'parameter': {'@name': 'Raw data file names',\n",
    "                                  'file': []}}\n",
    "    new_raw_data['parameter']['file'] = mzmine_things[i]['file_list']\n",
    "    mzmine_things[i]['param_dict_unflat']['batch']['batchstep'].insert(0,new_raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turn tabular parameter entry back into xml format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    t2 = mzm.dict_to_etree(mzmine_things[i]['param_dict_unflat'])\n",
    "    mzm.indent_tree(t2)\n",
    "    s2 = mzm.tree_to_xml(t2)\n",
    "    mzmine_things[i]['xml_string'] = s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Job Launch Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    mzmine_launcher = mzm.get_latest_mzmine_binary(version=m['mzmine_version'])\n",
    "    mzmine_things[i]['mzmine_launcher'] = mzmine_launcher\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    if not os.path.isdir(m['basedir']):\n",
    "        os.mkdir(m['basedir'])\n",
    "    batch_filename = '%s_batch-params.xml'%new_basename\n",
    "    with open(batch_filename,'w') as fid:\n",
    "        fid.write('%s'%m['xml_string'])\n",
    "    mzmine_things[i]['batch_filename'] = batch_filename\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save GNPS metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'20190918'),\n",
       " (1, u'AG'),\n",
       " (2, u'VB'),\n",
       " (3, u'BioSFA'),\n",
       " (4, u'AlgC18test'),\n",
       " (5, u'Pilot'),\n",
       " (6, u'QE144'),\n",
       " (7, u'EPC18'),\n",
       " (8, u'USDAY46920'),\n",
       " (9, u'POS'),\n",
       " (10, u'MSMS-v2'),\n",
       " (11, u'9'),\n",
       " (12, u'exud-MeOH'),\n",
       " (13, u'1'),\n",
       " (14, u'37.mzML')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # s = []\n",
    "# for m in mzmine_things[21]['file_list']:\n",
    "[(i,g) for i,g in enumerate(os.path.basename(mzmine_things[0]['file_list'][0]).split('_'))]\n",
    "#     print('-'.join(os.path.basename(m).split('_')[9].split('-')[:-1]))\n",
    "#     print('')\n",
    "# # sorted(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_positive_F379EF3C')\n",
      "(10, [u'exud-MeOH', u'ExCtrl-cart', u'ExCtrl-EtOAc', u'exud-EtOAc', u'ExCtrl-cart', u'exud-cart', u'exud-EtOAc', u'exud-MeOH', u'ExCtrl-MeOH', u'pell-EtOAc', u'pell-cart', u'ExCtrl-MeOH', u'pell-MeOH', u'QC-PrimMet-SOPv3', u'pell-MeOH', u'ExCtrl-EtOAc', u'exud-cart', u'pell-cart', u'pell-EtOAc'])\n",
      "\n",
      "(1, '20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_negative_55220FEA')\n",
      "(10, [u'ExCtrl-cart', u'exud-EtOAc', u'QC-PrimMet-SOPv3', u'exud-MeOH', u'ExCtrl-MeOH', u'pell-cart', u'ExCtrl-cart', u'pell-MeOH', u'ExCtrl-MeOH', u'exud-MeOH', u'ExCtrl-EtOAc', u'ExCtrl-EtOAc', u'pell-EtOAc', u'exud-cart', u'exud-cart', u'exud-EtOAc', u'pell-cart', u'pell-MeOH', u'pell-EtOAc'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    print(i,m['unique_basename'])\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    metadata_filename = '%s_%s.tab'%(new_basename,'metadata')\n",
    "    mzml_files = [os.path.basename(f) for f in m['file_list']]\n",
    "    if '20181109TV_' in m['unique_basename']:\n",
    "        attributes = [''.join(f.split('_')[9].split('-')[:-1]) for f in mzml_files]\n",
    "    elif '20161212_TS_BSC_EthAc_extracts' in m['unique_basename']:\n",
    "        attributes = [''.join(f.split('_')[11]) for f in mzml_files]            \n",
    "    elif '20181116_TV' in m['unique_basename']:\n",
    "        attributes = [''.join(f.split('_')[11].split('-')[:-1]) for f in mzml_files]        \n",
    "    elif '20151019_KZ_RootExu_C18_QExactive' in m['unique_basename']:\n",
    "        attributes = ['_'.join(f.split('_')[8:10]) for f in mzml_files]\n",
    "    elif '_AS_' in m['unique_basename']:\n",
    "        attributes = ['-'.join(f.split('_')[11].split('-')[:]) for f in mzml_files]\n",
    "    elif ('_MO' in m['unique_basename']) or ('candice' in m['unique_basename']):\n",
    "        attributes = ['-'.join(f.split('_')[12].split('-')[:-1]) for f in mzml_files]\n",
    "    elif '20190729_AK_JE_Ekwealor' in m['unique_basename']:\n",
    "        attributes = [f.split('_')[10] for f in mzml_files]\n",
    "    elif '20171214_KBL_JL_C18_Poplar_FungInnoc' in m['unique_basename']:\n",
    "        attributes = [f.split('_')[11] for f in mzml_files]\n",
    "    elif '20190829_AK_NS_ENIGMA_bsc_community_QEHF_Ag679775-924_USHXH0126' in m['unique_basename']:\n",
    "        attributes = [f.split('_')[10] for f in mzml_files]\n",
    "    else:\n",
    "        attributes = [f.split('_')[12] for f in mzml_files]\n",
    "#     except:\n",
    "#         attributes = ['no group' for f in mzml_files]\n",
    "    print(len(pd.unique(attributes)),attributes)\n",
    "    pd.DataFrame(data={'filename':mzml_files,'ATTRIBUTE_sampletype':attributes}).set_index('filename',drop=True).to_csv(metadata_filename,sep='\\t')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write each xml file to disk and test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/mzmine_parameters/MZmine/MZmine-2.39/startMZmine_NERSC_Headless_Cori.sh /global/cscratch1/sd/bpb/mzmine_jobs/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_positive_F379EF3C/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_positive_F379EF3C_batch-params.xml\n",
      "/global/common/software/m2650/mzmine_parameters/MZmine/MZmine-2.39/startMZmine_NERSC_Headless_Cori.sh /global/cscratch1/sd/bpb/mzmine_jobs/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_negative_55220FEA/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_negative_55220FEA_batch-params.xml\n"
     ]
    }
   ],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    print('%s %s'%(m['mzmine_launcher'],m['batch_filename']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make JAWS formated json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = json.dumps([b['batch_filename'] for b in mzmine_things])\n",
    "d = {\"jgi_mzmine.inputs\":[b['batch_filename'] for b in mzmine_things]}\n",
    "with open('/global/homes/b/bpb/mzmine_job_adap.json','w') as fid:\n",
    "    fid.write('%s'%json.dumps(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make SBATCH files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH -t 04:00:00\n",
      "#SBATCH -C haswell\n",
      "#SBATCH -N 1\n",
      "#SBATCH --error=\"slurm.err\"\n",
      "#SBATCH --output=\"slurm.out\"\n",
      "#SBATCH -q realtime\n",
      "#SBATCH -A m1541\n",
      "#SBATCH --exclusive\n",
      "module load java\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mzm = reload(mzm)\n",
    "print(mzm.SLURM_HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch /global/cscratch1/sd/bpb/mzmine_jobs/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_positive_F379EF3C/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_positive_F379EF3C_sbatch.sbatch\n",
      "sbatch /global/cscratch1/sd/bpb/mzmine_jobs/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_negative_55220FEA/20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_negative_55220FEA_sbatch.sbatch\n"
     ]
    }
   ],
   "source": [
    "mzm = reload(mzm)\n",
    "for i,m in enumerate(mzmine_things):\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    sbatch_filename = '%s_%s.sbatch'%(new_basename,'sbatch')\n",
    "    s = '%s %s'%(m['mzmine_launcher'],m['batch_filename'])\n",
    "    sbatch_header = mzm.SLURM_HEADER\n",
    "    \n",
    "    with open(sbatch_filename,'w') as fid:\n",
    "        fid.write('%s\\n%s\\n'%(mzm.SLURM_HEADER.replace('slurm',new_basename),s))\n",
    "    print('sbatch %s'%sbatch_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %system sbatch /global/cscratch1/sd/bpb/mzmine_jobs/20190418_JJ_RM_Garden_v2EcoFAB_2019Apr_QE144_Ag68377-924_USHXG01160_positive_BD1728C6/20190418_JJ_RM_Garden_v2EcoFAB_2019Apr_QE144_Ag68377-924_USHXG01160_positive_BD1728C6_sbatch.sbatch\n",
    "\n",
    "# %system sbatch /global/cscratch1/sd/bpb/mzmine_jobs/20190418_JJ_RM_Garden_v2EcoFAB_2019Apr_QE144_Ag68377-924_USHXG01160_negative_E828977F/20190418_JJ_RM_Garden_v2EcoFAB_2019Apr_QE144_Ag68377-924_USHXG01160_negative_E828977F_sbatch.sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%system squeue -u bpb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send each job to GNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# task_id = '0d7e35bb071b4679996580af5ef0abe7'\n",
    "# positive_url = \"https://gnps.ucsd.edu/ProteoSAFe/DownloadResultFile?task=%s&block=main&file=gnps_molecular_network_graphml/\" % (task_id)\n",
    "# positive_graphml = os.path.join('/global/homes/b/bpb/Downloads/', \"candice_combo.graphml\")\n",
    "\n",
    "# local_file = open(positive_graphml, \"w\")\n",
    "# local_file.write(requests.get(positive_url).text)\n",
    "# local_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/IPython/kernel/__main__.py:4: DeprecationWarning: Python 2 suport will be removed in ftputil 4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_positive_F379EF3C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/IPython/kernel/__main__.py:87: DeprecationWarning: `use_list_a_option` will default to `False` in ftputil 4.x.x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAKING DIR\n",
      "MAKING Group DIR\n",
      "MAKING Group DIR\n",
      "MAKING Group DIR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252f48fa7db54db797db3fba2a2b4025\n",
      "Launched Task: : 252f48fa7db54db797db3fba2a2b4025\n",
      "(0, u'252f48fa7db54db797db3fba2a2b4025')\n",
      "\n",
      "20190918_AG_VB_BioSFA_AlgC18test_Pilot_QE144_EPC18_USDAY46920_negative_55220FEA\n",
      "MAKING DIR\n",
      "MAKING Group DIR\n",
      "MAKING Group DIR\n",
      "MAKING Group DIR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n",
      "/global/common/software/m2650/python-cori/lib/python2.7/site-packages/urllib3/connectionpool.py:858: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cef9aa2683944829a734609f95a857e7\n",
      "Launched Task: : cef9aa2683944829a734609f95a857e7\n",
      "(1, u'cef9aa2683944829a734609f95a857e7')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(1,'/global/homes/b/bpb/repos/GNPS_quickstart/')\n",
    "import uuid\n",
    "execfile('/global/homes/b/bpb/repos/GNPS_quickstart/util.py')\n",
    "def copy_and_submit_to_gnps(m,override=False):\n",
    "    \n",
    "    \n",
    "    print(m['unique_basename'])\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    taskid_filename = '%s_%s.txt'%(new_basename,'gnps-uuid')\n",
    "    if (os.path.isfile(taskid_filename)) & (override==False):\n",
    "        print('Already submitted')\n",
    "        with open(taskid_filename,'r') as fid:\n",
    "            print(fid.read())\n",
    "        return False\n",
    "\n",
    "    metadata_filename = '%s_%s.tab'%(new_basename,'metadata')\n",
    "    if os.path.isfile(metadata_filename):\n",
    "        upload_to_gnps(metadata_filename,m['unique_basename'],'samplemetadata','bpbowen',gnps_password)\n",
    "    else:\n",
    "        print('METADATA NOT FOUND %s'%metadata_filename)\n",
    "        return False\n",
    "        \n",
    "    mgf_filename = '%s_%s.mgf'%(new_basename,'MSMS')\n",
    "    if os.path.isfile(mgf_filename):\n",
    "        upload_to_gnps(mgf_filename,m['unique_basename'],'featurems2','bpbowen',gnps_password)\n",
    "    else:\n",
    "        print('SPECTRA NOT FOUND %s'%mgf_filename)\n",
    "        return False\n",
    "        \n",
    "    quant_filename = '%s_%s.csv'%(new_basename,'peak-area')\n",
    "    if os.path.isfile(quant_filename):\n",
    "        upload_to_gnps(quant_filename,m['unique_basename'],'featurequantification','bpbowen',gnps_password)\n",
    "    else:\n",
    "        print('QUANT NOT FOUND %s'%quant_filename)\n",
    "        return False\n",
    "    \n",
    "    task_id = launch_GNPS_featurenetworking_workflow( m['unique_basename'],m['unique_basename'], \n",
    "                                           'bpbowen', gnps_password, 'ben.bowen@gmail.com',\n",
    "                                           'MZMINE2', [], 'HIGHRES',\n",
    "                                           os.path.basename(metadata_filename),\n",
    "                                           os.path.basename(mgf_filename),\n",
    "                                           os.path.basename(quant_filename),\n",
    "                                          uuid.uuid1()) #I don't think this uuid does anything\n",
    "    if task_id is not None:\n",
    "        with open(taskid_filename,'w') as fid:\n",
    "            fid.write('https://gnps.ucsd.edu/ProteoSAFe/status.jsp?task=%s'%task_id)\n",
    "        # SUCCESS!!!\n",
    "        return task_id\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "for i,m in enumerate(mzmine_things):\n",
    "    taskid = copy_and_submit_to_gnps(m,override=False)\n",
    "    print(i,taskid)\n",
    "    print('')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge pos and neg jobs into combined network\n",
    "\n",
    "https://github.com/mwang87/MergePolarity/blob/master/merge_polarity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basedir</th>\n",
       "      <th>unique_basename</th>\n",
       "      <th>polarity</th>\n",
       "      <th>task_id</th>\n",
       "      <th>stripped_basename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_763DE1F7</td>\n",
       "      <td>positive</td>\n",
       "      <td>a8a459391a51435db7b85b60e3b09cfb</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_4FF0A628</td>\n",
       "      <td>negative</td>\n",
       "      <td>98e6ae87c6074d7ea51975555221360a</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_6AD23F7C</td>\n",
       "      <td>positive</td>\n",
       "      <td>6345d02fd4ac498d8e8da8ec5664a6f4</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_CDBCC477</td>\n",
       "      <td>negative</td>\n",
       "      <td>84cbbe4cc8d34ce58e83d97423529d9d</td>\n",
       "      <td>20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_positive_7C475F10</td>\n",
       "      <td>20161212_TS_BSC_EthAc_extracts_positive_7C475F10</td>\n",
       "      <td>positive</td>\n",
       "      <td>00a5f481f5434c84bccce13752b347ce</td>\n",
       "      <td>20161212_TS_BSC_EthAc_extracts_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_negative_63389345</td>\n",
       "      <td>20161212_TS_BSC_EthAc_extracts_negative_63389345</td>\n",
       "      <td>negative</td>\n",
       "      <td>dd10d9cfe6bc4ab5bca8a891e89c2b76</td>\n",
       "      <td>20161212_TS_BSC_EthAc_extracts_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               basedir                                                                             unique_basename  polarity                           task_id                                                          stripped_basename\n",
       "0  /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_763DE1F7  positive  a8a459391a51435db7b85b60e3b09cfb  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_\n",
       "1  /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_4FF0A628  negative  98e6ae87c6074d7ea51975555221360a  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_\n",
       "2  /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_6AD23F7C  positive  6345d02fd4ac498d8e8da8ec5664a6f4  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_\n",
       "3  /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC...  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_CDBCC477  negative  84cbbe4cc8d34ce58e83d97423529d9d  20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_\n",
       "4                /global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_positive_7C475F10                                            20161212_TS_BSC_EthAc_extracts_positive_7C475F10  positive  00a5f481f5434c84bccce13752b347ce                                            20161212_TS_BSC_EthAc_extracts_\n",
       "5                /global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_negative_63389345                                            20161212_TS_BSC_EthAc_extracts_negative_63389345  negative  dd10d9cfe6bc4ab5bca8a891e89c2b76                                            20161212_TS_BSC_EthAc_extracts_"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(mzmine_things)\n",
    "df = df[['basedir','unique_basename','polarity']]\n",
    "# df.drop(columns=['file_list','file_filters','groups','param_dict_flat','param_dict_unflat','xml_string','mzmine_launcher'],inplace=True)\n",
    "for i,m in df.iterrows():\n",
    "    new_basename = os.path.join(m['basedir'],m['unique_basename'])\n",
    "    taskid_filename = '%s_%s.txt'%(new_basename,'gnps-uuid')\n",
    "    if (os.path.isfile(taskid_filename)):\n",
    "        with open(taskid_filename,'r') as fid:\n",
    "            df.loc[i,'task_id'] = fid.read().split('=')[-1]\n",
    "    df.loc[i,'stripped_basename'] = '_'.join(m['unique_basename'].replace('positive','').replace('negative','').split('_')[:-1])\n",
    "#     df.loc[i,'stripped_basename'] = '_'.join(m['unique_basename'].replace('Pos','').replace('Neg','').split('_')[:-1])\n",
    "\n",
    "df.fillna('',inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module load python\n",
      "#20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_\n",
      "python /global/homes/b/bpb/repos/MergePolarity/merge_polarity.py --output-graphml /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_6AD23F7C/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_6AD23F7C_merged_network.graphml --positive-network-task 6345d02fd4ac498d8e8da8ec5664a6f4 --negative-network-task 84cbbe4cc8d34ce58e83d97423529d9d --RT_TOLERANCE 0.15\n",
      "#20161212_TS_BSC_EthAc_extracts_\n",
      "python /global/homes/b/bpb/repos/MergePolarity/merge_polarity.py --output-graphml /global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_positive_7C475F10/20161212_TS_BSC_EthAc_extracts_positive_7C475F10_merged_network.graphml --positive-network-task 00a5f481f5434c84bccce13752b347ce --negative-network-task dd10d9cfe6bc4ab5bca8a891e89c2b76 --RT_TOLERANCE 0.15\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "print('module load python')\n",
    "for i in df['stripped_basename'].unique():\n",
    "    if len(df[df['stripped_basename']==i].index)>1:\n",
    "        d[i] = {}\n",
    "        d[i]['positive'] = {}\n",
    "        d[i]['negative'] = {}\n",
    "        for j in df[df['stripped_basename']==i].index:\n",
    "            d[i][df['polarity'][j]]['job_id'] = df['task_id'][j]\n",
    "            d[i][df['polarity'][j]]['basedir'] = df['basedir'][j]\n",
    "            d[i][df['polarity'][j]]['unique_basename'] = df['unique_basename'][j]\n",
    "#         mydir = '/global/cscratch1/sd/bpb/mzmine_jobs/%s'%i\n",
    "        if (d[i]['positive']['job_id']!='') & (d[i]['negative']['job_id']!=''):\n",
    "            print('#%s'%i)\n",
    "#             if not os.path.isdir(mydir):\n",
    "#                 os.mkdir(mydir)\n",
    "            pos_outfile = os.path.join(d[i]['positive']['basedir'],'positive_network.graphml')\n",
    "            neg_outfile = os.path.join(d[i]['negative']['basedir'],'negative_network.graphml')\n",
    "            merged_outfile = os.path.join(d[i]['positive']['basedir'],'%s_merged_network.graphml'%d[i]['positive']['unique_basename'])\n",
    "            command = 'python /global/homes/b/bpb/repos/MergePolarity/merge_polarity.py'\n",
    "#              --positive-graphml %s --negative-graphml %s\n",
    "            print('%s --output-graphml %s --positive-network-task %s --negative-network-task %s --RT_TOLERANCE 0.15'%(\n",
    "                command,\n",
    "                merged_outfile,\n",
    "#                 pos_outfile,\n",
    "#                 neg_outfile,\n",
    "                d[i]['positive']['job_id'],\n",
    "                d[i]['negative']['job_id']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHGRP to metatlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing permissions for /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_763DE1F7\n",
      "changing permissions for /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_4FF0A628\n",
      "changing permissions for /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_6AD23F7C\n",
      "changing permissions for /global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_CDBCC477\n",
      "changing permissions for /global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_positive_7C475F10\n",
      "changing permissions for /global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_negative_63389345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    print(\"changing permissions for %s\"%m['basedir'])\n",
    "    os.system(\"chgrp -R metatlas %s\"%m['basedir'])\n",
    "os.system(\"chmod o+rx /global/cscratch1/sd/bpb\")\n",
    "os.system(\"chmod o+rx /global/cscratch1/sd/bpb/mzmine_jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_763DE1F7\n",
      "/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_4FF0A628\n",
      "/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_positive_6AD23F7C\n",
      "/global/cscratch1/sd/bpb/mzmine_jobs/20191004_AG_AO_JGI2ndMet_Kphytohabitans_SetMerged_QE144_EPC18_USDAY46920_negative_CDBCC477\n",
      "/global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_positive_7C475F10\n",
      "/global/cscratch1/sd/bpb/mzmine_jobs/20161212_TS_BSC_EthAc_extracts_negative_63389345\n"
     ]
    }
   ],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    print(\"%s\"%m['basedir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of MZMINE + GNPS + Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_consecutive(data,stepsize=10.0,do_ppm=True):\n",
    "    \"\"\"\n",
    "    split a numpy array where consecutive elements are greater than stepsize\n",
    "    can be ppm or value\n",
    "    if idx is not None then returns indices of elements otherwise returns values\n",
    "    \n",
    "    The main use case is an unsorted list of m/zs as \"data\" and optionally \n",
    "    their numerical index as \"idx\". Typically a user would want to retrieve \n",
    "    the group indices in the original order that they provided their list \n",
    "    of m/zs.\n",
    "    \n",
    "    usage:\n",
    "    \n",
    "       \n",
    "    \"\"\"\n",
    "    if type(data) is np.ndarray:\n",
    "        # cool way to sort and unsort array:\n",
    "        idx_sorted = data.argsort()\n",
    "        sort_w_unsort = np.column_stack((np.arange(idx_sorted.size),idx_sorted))\n",
    "        # sort_w_unsort[:,0] are the original indices of data\n",
    "        # sort_w_unsort[:,1] are the sorted indices of data\n",
    "        data_sorted = data[sort_w_unsort[:,1]]\n",
    "        # np.argsort(sort_w_unsort[:,1]) returns the indices to map the sorted data back to the original\n",
    "        # data_unsorted = data_sorted[np.argsort(sort_w_unsort[:,1])]\n",
    "        \n",
    "        if do_ppm:\n",
    "            d = np.diff(data_sorted) / data_sorted[:-1] * 1e6\n",
    "        else:\n",
    "            d = np.diff(data_sorted)\n",
    "        \n",
    "        # make groups of the array\n",
    "        data_groups = np.split(data_sorted, np.where(d > 2.0*stepsize)[0]+1)\n",
    "        # replace each group of values with group index\n",
    "        for i,data_slice in enumerate(data_groups):\n",
    "            data_groups[i] = data_groups[i]*0 + i\n",
    "        group_indices = np.concatenate(data_groups)\n",
    "        # reorder the group indices\n",
    "        group_indices = group_indices[np.argsort(sort_w_unsort[:,1])]\n",
    "        return group_indices.astype(int)#\n",
    "    else:\n",
    "        print('not a numpy array. convert it and sort it first')\n",
    "\n",
    "def map_mzgroups_to_data(mz_atlas,mz_group_indices,mz_data):\n",
    "    \"\"\"\n",
    "    mz_atlas: m/z values from atlas\n",
    "    mz_group_indices: integer index from \"group_consecutive\"\n",
    "    mz_data: m/z values from raw data\n",
    "    \n",
    "    \"\"\"\n",
    "    from scipy import interpolate\n",
    "\n",
    "    f = interpolate.interp1d(mz_atlas,np.arange(mz_atlas.size),kind='nearest',bounds_error=False,fill_value='extrapolate') #get indices of all mz values in the atlas\n",
    "    idx = f(mz_data)   # iterpolate to find the nearest mz in the data for each mz in an atlas\n",
    "    idx = idx.astype('int')\n",
    "#     d = 1e6#np.abs(mz_data - mz_atlas[idx]) / mz_data * 1.0e6\n",
    "#     output_mat = np.column_stack((d,))\n",
    "    return mz_group_indices[idx]#output_mat\n",
    "\n",
    "g = map_mzgroups_to_data(atlas['mz'].values[:],\n",
    "                           atlas['group_index'].values[:],\n",
    "                           msdata['mz'].values[:])\n",
    "msdata['group_index'] = g#[:,1]\n",
    "#     msdata['group_index_ppm'] = g[:,0]\n",
    "\n",
    "df = pd.merge(atlas,msdata,left_on='group_index',right_on='group_index',how='outer',suffixes=('_atlas','_data'))\n",
    "\n",
    "#grab all datapoints including \"extra\"\n",
    "mz_condition = abs(df['mz_data']-df['mz_atlas'])/df['mz_atlas']*1e6<df['ppm_tolerance']\n",
    "rt_min_condition = df['rt']>=(df['rt_min']-df['extra_time'])\n",
    "rt_max_condition = df['rt']<=(df['rt_max']+df['extra_time'])\n",
    "df = df[(mz_condition) & (rt_min_condition) & (rt_max_condition)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %system cat /project/projectdirs/metatlas/projects/jgi_projects/20190522_KBL_JM_504478_Plant_Arab_QE-HF_C18_USDAY47560_positive_1F910A37/20190522_KBL_JM_504478_Plant_Arab_QE-HF_C18_USDAY47560_positive_1F910A37_batch-params.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/project/projectdirs/metatlas/projects/jgi_projects/20190522_KBL_JM_504478_Plant_Arab_QE-HF_C18_USDAY47560_positive_1F910A37/20190522_KBL_JM_504478_Plant_Arab_QE-HF_C18_USDAY47560_positive_1F910A37_batch-params.xml','r') as fid:\n",
    "#     s = fid.read()\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(new_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mzmine_things[0]\n",
    "with open('/global/homes/b/bpb/Downloads/gnps_section_mzmine_batch.xml','r') as fid:\n",
    "    d = fid.read()\n",
    "new_basename = os.path.join(m['basedir'],'%s_%s'%(m['unique_basename'],'msms.mgf'))\n",
    "d = d.replace('/global/homes/b/bpb/Downloads/gnps_upload.mgf',new_basename)\n",
    "print(d)\n",
    "new_batch = os.path.join(m['basedir'],'%s_%s'%(m['unique_basename'],'gnps-submission.xml'))\n",
    "with open(new_batch,'w') as fid:\n",
    "    fid.write(d)\n",
    "mzmine_launcher = mzm.get_latest_mzmine_binary(version=m['mzmine_version'])\n",
    "print('%s %s'%(mzmine_launcher,new_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,m in enumerate(mzmine_things):\n",
    "    print('%s %s'%(m['mzmine_launcher'],m['batch_filename']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see if your jobs finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%system cat \"/project/projectdirs/metatlas/projects/jgi_projects/20190522_KBL_JM_504478_Plant_Arab_QE-HF_C18_USDAY47560_positive_13EEF8DE/20190522_KBL_JM_504478_Plant_Arab_QE-HF_C18_USDAY47560_positive_13EEF8DE_peak-height.csv\" | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = dp.get_google_sheet(notebook_name='mzmine_jobs_20190625',sheet_name='JOBS',literal_cols=['file_filters','groups'])\n",
    "# mzmine_things = df.T.to_dict().values()\n",
    "import os.path, time\n",
    "\n",
    "for i,m in enumerate(mzmine_things):\n",
    "#     new_basename = os.path.join(m['basedir'],'%s_%s'%(m['unique_basename'],'peak-height.csv'))\n",
    "#     new_basename = os.path.join(m['basedir'],'%s_%s'%(m['unique_basename'],'msms.mgf'))\n",
    "    new_basename = os.path.join(m['basedir'],'%s%s'%(m['unique_basename'],'.err'))\n",
    "\n",
    "    print(i,os.path.isfile(new_basename),new_basename)\n",
    "    print(\"last modified: %s\" % time.ctime(os.path.getmtime(new_basename)))\n",
    "    with open(new_basename,'r') as fid:\n",
    "        s = fid.read()\n",
    "    print(s[-100:])\n",
    "    print('')\n",
    "#     df = pd.read_csv(new_basename)\n",
    "#     print(df.shape)\n",
    "#     print(os.listdir(m['basedir']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# turn profile into centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzmine_things[0]['file_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_mgf_text(t):\n",
    "    data = [(tt.strip().split('\\n')[:7],tt.strip().split('\\n')[7:][:-1]) for tt in t.split('END IONS')]\n",
    "    mgf_str = ''\n",
    "    for data_slice in data:\n",
    "        spectrum = [(float(dd.split(' ')[0]),float(dd.split(' ')[-1])) for dd in data_slice[-1]]\n",
    "        peak_max,peak_min = sp.peakdet([s[1] for s in spectrum],10.0,[s[0] for s in spectrum])\n",
    "        header = '\\n'.join(data_slice[0])\n",
    "        spec_str = '\\n'.join(['%5.4f %5.1f'%(p[0],p[1]) for p in peak_max])\n",
    "        mgf_str = '%s\\n\\n%s\\n%s\\nEND IONS\\n'%(mgf_str,header,spec_str)\n",
    "    return mgf_str\n",
    "\n",
    "\n",
    "for i,m in enumerate(mzmine_things):\n",
    "    filename = os.path.join(m['basedir'],'%s_%s'%(m['unique_basename'],'MSMS.mgf'))\n",
    "    with open(filename,'r') as fid:\n",
    "        t = fid.read()\n",
    "        mgf_text = make_new_mgf_text(t)\n",
    "    new_filename = os.path.join(m['basedir'],'%s_%s'%(m['unique_basename'],'MSMS-centroid.mgf'))\n",
    "    print new_filename\n",
    "    with open(new_filename,'w') as fid:\n",
    "        fid.write('%s'%mgf_text)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from metatlas.helpers import spectralprocessing as sp\n",
    "peak_max,peak_min = sp.peakdet([s[1] for s in spectrum],10.0,[s[0] for s in spectrum])\n",
    "fig,ax = plt.subplots()\n",
    "ax.set_title(len(peak_max))\n",
    "ax.vlines([x[0] for x in peak_max],[0.0 for x in peak_max],[x[1] for x in peak_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare results to true positives table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzm = reload(mzm)\n",
    "df = dp.get_google_sheet(notebook_name='mzmine_jobs_20190625',sheet_name='JOBS',literal_cols=['file_filters','groups'])\n",
    "df = df[df['done']==False]\n",
    "\n",
    "mzmine_things = df.T.to_dict().values()\n",
    "mz_tolerance = 0.01\n",
    "rt_tolerance = 0.2\n",
    "\n",
    "true_pos_filename = '/global/homes/b/bpb/Downloads/mana_bootcamp_true_positives.xlsx'\n",
    "\n",
    "# base_path = '/project/projectdirs/metatlas/projects/jgi_projects/'\n",
    "base_path = '/global/cscratch1/sd/bpb/mzmine_jobs/'\n",
    "\n",
    "# project_path = '20190401_JGI_Metabolomics_Bootcamp_ReversePhase_18DCF58D'\n",
    "# project_path = '20190401_JGI_Metabolomics_Bootcamp_HILIC_D34FE46C'\n",
    "# project_path = '20190401_JGI_Metabolomics_Bootcamp_ReversePhase_positive_2800C348'\n",
    "project_path = '20190401_JGI_Metabolomics_Bootcamp_ReversePhase_positive_4E39E174'\n",
    "# project_path = '20190501_bootcamp_PeterAndeer'\n",
    "# project_path = '20190501_bootcamp_BrianDeFelice'\n",
    "\n",
    "feature_table_extension = 'height.csv' # for mzmine files\n",
    "rt_column = 'row retention time'\n",
    "mz_column = 'row m/z'\n",
    "headerrows = 0\n",
    "sep=','\n",
    "n=4\n",
    "# feature_table_extension = '.txt' # for mzmine files\n",
    "# rt_column = 'Average Rt(min)'\n",
    "# mz_column = 'Average Mz'\n",
    "# headerrows = 3\n",
    "# sep='\\t'\n",
    "data = []\n",
    "for i,m in enumerate(mzmine_things):\n",
    "    bd = m['basedir'].split('/')[-1]\n",
    "    if 'HILIC' in bd.upper():\n",
    "        chromatography = 'hilic'\n",
    "    else:\n",
    "        chromatography = 'reverse phase'\n",
    "    istd,bio,total = mzm.summarize_results(n,true_pos_filename,base_path,bd,feature_table_extension,rt_column,mz_column,headerrows,sep,mz_tolerance,rt_tolerance)\n",
    "    d = {'job':bd,'type':'istd','chromatography':chromatography,'job_type':'%s_%s'%(bd,'istd'),'total':total}\n",
    "    for j,h in enumerate(istd):\n",
    "        d['num_%d'%j] = h\n",
    "    data.append(d)\n",
    "    d = {'job':bd,'type':'bio','chromatography':chromatography,'job_type':'%s_%s'%(bd,'bio'),'total':total}\n",
    "    for j,h in enumerate(bio):\n",
    "        d['num_%d'%j] = h\n",
    "    data.append(d)\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index('job_type',drop=True,inplace=True)\n",
    "df.head()\n",
    "    \n",
    "#     print(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = [i[-30:-5] for i in df.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE DEDUPLICATOR IS LIKELY REMOVING PEAKS THAT ARE REALLY DIFFERENT PEAKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mzm = reload(mzm)\n",
    "width = 0.4\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2,ncols=2,figsize=(21,15))\n",
    "cols = [c for c in df.columns if 'num_' in c]\n",
    "df_f = df[(df['type']=='bio') & (df['chromatography']=='reverse phase')]\n",
    "df_f[cols].plot(kind='bar', sharex=False, sharey=False, stacked=True,ax=ax[0,0],width=width,position=0)\n",
    "ax[0,0].set_yticks(np.linspace(0,1,11))\n",
    "ax[0,0].grid(True)\n",
    "ax[0,0].legend(loc='upper center',fontsize=20)\n",
    "ax[0,0].set_title('Bio / Reverse-Phase')\n",
    "ax[0,0].set_ylabel('Percent Correct')\n",
    "ax2 = ax[0,0].twinx() # Create another axes that shares the same x-axis as ax.\n",
    "df_f['total'].plot(kind='bar',ax=ax2, sharex=True, sharey=False, color='purple', width=width, position=1)\n",
    "ax2.legend(loc='lower center',fontsize=20)\n",
    "ax2.set_ylabel('Num Features')\n",
    "ax2.set_xlim(-1,df_f.shape[0])\n",
    "\n",
    "# ax2.set_xticklabels([i for i in df_f.index])\n",
    "\n",
    "\n",
    "df_f = df[(df['type']=='bio') & (df['chromatography']=='hilic')]\n",
    "df_f[cols].plot(kind='bar', stacked=True,ax=ax[0,1], width=width,position=0)\n",
    "ax[0,1].set_yticks(np.linspace(0,1,11))\n",
    "ax[0,1].grid(True)\n",
    "ax[0,1].get_legend().remove()\n",
    "ax[0,1].set_title('Bio / HILIC')\n",
    "ax2 = ax[0,1].twinx() # Create another axes that shares the same x-axis as ax.\n",
    "df_f['total'].plot(kind='bar',ax=ax2, sharex=True, sharey=False, color='purple', width=width, position=1)\n",
    "ax2.set_ylabel('Num Features')\n",
    "ax2.set_xlim(-1,df_f.shape[0])\n",
    "ax[0,1].legend(loc='upper center',fontsize=20)\n",
    "\n",
    "\n",
    "df_f = df[(df['type']=='istd') & (df['chromatography']=='reverse phase')]\n",
    "df_f[cols].plot(kind='bar', stacked=True,ax=ax[1,0], width=width,position=0)\n",
    "ax[1,0].set_yticks(np.linspace(0,1,11))\n",
    "ax[1,0].grid(True)\n",
    "ax[1,0].get_legend().remove()\n",
    "ax[1,0].set_title('ISTD / Reverse-Phase')\n",
    "ax2 = ax[1,0].twinx() # Create another axes that shares the same x-axis as ax.\n",
    "df_f['total'].plot(kind='bar',ax=ax2, sharex=True, sharey=False, color='purple', width=width, position=1)\n",
    "ax2.set_ylabel('Num Features')\n",
    "ax2.set_xlim(-1,df_f.shape[0])\n",
    "ax[1,0].legend(loc='upper center',fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "df_f = df[(df['type']=='istd') & (df['chromatography']=='hilic')]\n",
    "df_f[cols].plot(kind='bar', stacked=True,ax=ax[1,1], width=width,position=0)\n",
    "ax[1,1].set_yticks(np.linspace(0,1,11))\n",
    "ax[1,1].grid(True)\n",
    "ax[1,1].get_legend().remove()\n",
    "ax[1,1].set_title('ISTD / HILIC')\n",
    "ax2 = ax[1,1].twinx() # Create another axes that shares the same x-axis as ax.\n",
    "df_f['total'].plot(kind='bar',ax=ax2, sharex=True, sharey=False, color='purple', width=width, position=1)\n",
    "ax2.set_ylabel('Num Features')\n",
    "ax2.set_xlim(-1,df_f.shape[0])\n",
    "ax[1,1].legend(loc='upper center',fontsize=20)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out the results and see if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzm = reload(mzm)\n",
    "df = dp.get_google_sheet(notebook_name='mzmine_jobs_20190625',sheet_name='JOBS',literal_cols=['file_filters','groups'])\n",
    "# mzmine_things = df.T.to_dict().values()\n",
    "job = df.loc[44,:]\n",
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(job['basedir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make GNPS metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_basename = os.path.join(job['basedir'],job['unique_basename'])\n",
    "peak_height_filename = '%s_%s.csv'%(new_basename,'peak-height')\n",
    "metadata_filename = '%s_%s.tab'%(new_basename,'metadata')\n",
    "\n",
    "df = pd.read_csv(peak_height_filename)\n",
    "mzml_files = [c.replace(' Peak height','') for c in df.columns if 'Peak height' in c]\n",
    "pd.DataFrame(data={'filename':mzml_files,'ATTRIBUTE_sampletype':[m.split('_')[12] for m in mzml_files]}).set_index('filename',drop=True).to_csv(metadata_filename,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup some specific things for each set not defined in the mzmine params recipe\n",
    "\n",
    "1. polarity for filtering any fps files that might be loaded up\n",
    "2. file list for processing\n",
    "3. export file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('/project/projectdirs/metatlas/projects/mzmine_parameters/batch_files/bootcamp_template_batch_file.xml','r') as fid:\n",
    "#     xml_str = fid.read()\n",
    "    \n",
    "# d = mzm.xml_to_dict(xml_str)\n",
    "# t = mzm.dict_to_etree(d)\n",
    "# mzm.indent_tree(t)\n",
    "# s1 = mzm.tree_to_xml(t)\n",
    "\n",
    "\n",
    "# df = mzm.flatten(d,enumerate_types=(list,))\n",
    "# #sorted(['.'.join(tuple(str(x) for x in tup)) for tup in df.keys()])\n",
    "# df2 = mzm.unflatten(df)\n",
    "# # t2 = mzm.dict_to_etree(xml_2)\n",
    "# # mzm.indent_tree(t2)\n",
    "# # s2 = mzm.tree_to_xml(t2)\n",
    "\n",
    "\n",
    "\n",
    "# # t = dict_to_etree(new_d)\n",
    "# #     indent_tree(t)\n",
    "# #     xml_batch_str = tree_to_xml(t,filename=task.input_xml)\n",
    "# #     job_runner = '%s %s'%(task.mzmine_launcher,task.input_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dp.get_google_sheet(notebook_name='ADAP Bootcamp MZMine Untargeted Job Submission',sheet_name='min_max')\n",
    "s = df.iloc[0]\n",
    "\n",
    "test_params = {y:x for x,y in s.to_dict().iteritems() if len(x)>0 }\n",
    "\n",
    "for k,v in test_params.iteritems():\n",
    "    v2 = literal_eval(v)\n",
    "#     test_params[k] = {'min':v2[0],'max':v2[1],'n':v2[2],'vals':np.linspace(v2[0],v2[1],num=v2[2])}\n",
    "    test_params[k] = np.linspace(v2[0],v2[1],num=v2[2])\n",
    "\n",
    "df.columns = df.iloc[0]\n",
    "df = df.iloc[1:]\n",
    "\n",
    "# do ast.literal_eval on lists\n",
    "literal_cols = ['file_filters','groups']\n",
    "for col in literal_cols:\n",
    "    df[col] = df[col].apply(literal_eval)\n",
    "\n",
    "df['is_group'] = [bool(strtobool(a)) for a in df['is_group']]\n",
    "df['remove_isotopes'] = [bool(strtobool(a)) for a in df['remove_isotopes']]\n",
    "df['peak_with_msms'] = [bool(strtobool(a)) for a in df['peak_with_msms']]\n",
    "\n",
    "numeric_columns = [ u'max_peak_duration', u'min_rt', u'max_rt', u'min_peak_duration',u'min_rt', u'max_rt', u'min_peak_duration', u'max_peak_duration', u'smoothing_scans', u'min_num_scans', u'group_intensity_threshold',\n",
    "                   u'min_peak_height', u'mz_tolerance', u'peak_to_valley_ratio', u'rt_tol_multifile',\n",
    "                   u'rt_tol_perfile', u'ms1_noise_level', u'ms2_noise_level', u'chromatographic_threshold', \n",
    "                   u'search_for_minimum_rt_range', u'minimum_relative_height', u'mz_range_scan_pairing',\n",
    "                   u'rt_range_scan_pairing','min_peaks_in_row','gapfill_intensity_tolerance',\n",
    "                  'adap_sn_threshold','adap_peak_width_mult','adap_area_threshold','adap_rt_wavelet_min','adap_rt_wavelet_max']\n",
    "\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "mzmine_things = df.T.to_dict().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups = ['20160825_KBL_C18_MP_Solar_Berk','20170728_KBL_C18_MP_Solar_Spain']\n",
    "mzm = reload(mzm)\n",
    "current_month = datetime.now().strftime('%m') #// 02 //This is 0 padded\n",
    "current_year = datetime.now().strftime('%Y')  #// 2018\n",
    "year_month_dir = '%s%s'%(current_year,current_month)\n",
    "for i,m in enumerate(mzmine_things):\n",
    "    mzmine_things[i]['files'] = mzm.get_files(m['groups'],m['filename_substring'],m['file_filters'],m['is_group'])\n",
    "    print(m['basename'],m['polarity'],len(mzmine_things[i]['files']))\n",
    "    new_dir = os.path.join(mzm.DATA_PATH,year_month_dir)\n",
    "    if not os.path.isdir(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    for j,f in enumerate(mzmine_things[i]['files']):    \n",
    "        new_file_path = os.path.join(new_dir,os.path.basename(f))\n",
    "        if not os.path.isfile(new_file_path):\n",
    "            copyfile(f, new_file_path)\n",
    "        mzmine_things[i]['files'][j] = new_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_variables = sorted(test_params) \n",
    "combinations = list(product(*(test_params[var] for var in all_variables)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each mzmine thing, and for each parameter, make a new mzmine thing with that parameter\n",
    "new_mzmine_things = []\n",
    "for i,m in enumerate(mzmine_things): #iterate across hilic and rp\n",
    "    counter = 0\n",
    "    for values in combinations: #iterate through all the possible combinations\n",
    "        new_mzmine_things.append(copy.deepcopy(m)) #append all the static info from an mzmine_thing\n",
    "        new_mzmine_things[-1]['basename'] = '%d_%s'%(counter,m['basename']) #give the output files a unique filename\n",
    "        new_mzmine_things[-1]['unique_basename'] = '%d_%s'%(counter,m['unique_basename']) #this might not be used. I can't remember.\n",
    "        counter += 1\n",
    "        for j,var in enumerate(all_variables): #iterate across the mzmine variables that need to be changed\n",
    "            new_mzmine_things[-1][var] = values[j] #use the new value for a variable\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "status = []\n",
    "for i,params in enumerate(new_mzmine_things):\n",
    "    project_name = '%s_%s'%(params['unique_basename'],params['polarity'])\n",
    "    results_dir = os.path.join(params['basedir'],project_name)\n",
    "    peak_height_file = os.path.join(params['basedir'],'%s_%s_peak_height.csv'%(params['basename'],params['polarity']))\n",
    "    # see if final mzmine feature table exists.\n",
    "    job_done_1 = os.path.isfile(peak_height_file)\n",
    "    # if mzmine workspace exists store this so it can be removed from job script for reruns\n",
    "    new_mzmine_things[i]['mzmine_done'] = job_done_1\n",
    "    \n",
    "    # see if sheets/peak_height.tab exists\n",
    "    job_done_2 = False\n",
    "    identification_folder = os.path.join(results_dir,'identification')\n",
    "    num_id_figures = 0\n",
    "    if os.path.isfile(peak_height_file):\n",
    "        with open(peak_height_file,'r') as fid:\n",
    "            peaks = fid.read()\n",
    "            num_peaks = len(peaks.split('\\n'))-1\n",
    "        num_id_figures = len(glob.glob(os.path.join(identification_folder,'*.pdf')))\n",
    "        if num_peaks > 0:\n",
    "            job_done_2 = True\n",
    "    else:\n",
    "        num_peaks = 0\n",
    "    # if metatlas is done note this too, but these jobs shouldn't be rerun (since they are done)\n",
    "    new_mzmine_things[i]['metatlas_done'] = job_done_2\n",
    "    \n",
    "    # see if the filtered mzmine workspace has been created\n",
    "    job_done_3 = os.path.isfile(os.path.join(results_dir,'%s.mzmine'%project_name))\n",
    "    # this won't change the job script but completes the book-keeping\n",
    "    new_mzmine_things[i]['small_mzmine_done'] = job_done_3\n",
    "    \n",
    "    status.append({'0_basedir':params['basedir'].split('/')[-1],\n",
    "                   '1_job':'%s_%s'%(params['unique_basename'],params['polarity']),\n",
    "                   '2_features_done':job_done_1,\n",
    "                  '3_num_features':num_peaks})\n",
    "pd.DataFrame(status).to_csv('/global/homes/b/bpb/Downloads/mzmine_jobs_and_status.csv',index=False)    \n",
    "# pd.DataFrame(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_mzmine_things=[]\n",
    "# print(len(mzmine_things))\n",
    "# for i,params in enumerate(mzmine_things):\n",
    "#     # see if final mzmine file exists.  this will happen even if peak_height.tab is missing\n",
    "#     if (not params['metatlas_done']) or (not params['small_mzmine_done']) or (not params['mzmine_done']):\n",
    "#         new_mzmine_things.append(params)\n",
    "# mzmine_things = new_mzmine_things\n",
    "# print(len(mzmine_things))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in pd.unique([m['basedir'] for m in new_mzmine_things]):\n",
    "#     print('rm -r %s'%m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in pd.unique([m['basedir'] for m in new_mzmine_things]):\n",
    "#     print('chmod -R 770 %s'%m)\n",
    "#     print('chgrp -R metatlas %s'%m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mzm = reload(mzm)\n",
    "def remove_workspace_from_job(job_filename):\n",
    "    with open(job_filename,'r') as fid:\n",
    "        s = fid.read()\n",
    "\n",
    "    split_str = '</batchstep>\\n'\n",
    "    s = s.split(split_str)\n",
    "    idx = [i for i,ss in enumerate(s) if 'peak_area' in ss][-1]\n",
    "    s = s[:idx]\n",
    "    s.append('</batch>\\n')\n",
    "    s = split_str.join(s)\n",
    "    with open(job_filename,'w') as fid:\n",
    "        fid.write('%s'%s)\n",
    "\n",
    "\n",
    "with open('/global/homes/b/bpb/bootcamp/bootcamp_jobs.sh','w') as bcf:\n",
    "    for i,params in enumerate(new_mzmine_things):\n",
    "        #setup all the job scripts when job actually runs it will copy params to params-used\n",
    "        job_script = mzm.create_job_script(params)\n",
    "\n",
    "        \n",
    "#         wipe out the peak area, mgf, and workspace export\n",
    "        with open(job_script,'r') as fid:\n",
    "            s = fid.read()\n",
    "        job_filename = s.split(' ')[-1].strip()\n",
    "        remove_workspace_from_job(job_filename)\n",
    "        \n",
    "        #dump the parameters to a json file\n",
    "        with open( os.path.join(params['basedir'],'logs','%s_%s_params.json'%(params['basename'],params['polarity'])), 'w') as fid:\n",
    "            fid.write(json.dumps(params))\n",
    "        with open(job_script,'r') as fid:\n",
    "            s = fid.read()\n",
    "        bcf.write('%s\\n'%s.split('\\n')[-2])\n",
    "        \n",
    "    #     print('sbatch %s'%job_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chgrp -R metatlas mzmine_jobs/ADAP_20190401_JGI_Metabolomics_Bootcamp_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24000*10/4/200/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "d = '/global/cscratch1/sd/jaws/prod/cromwell-executions/jgi_mzmine/d6a9b53b-fe5b-4782-b7c0-d019573eb9e7/call-run_mzmine'\n",
    "for i in range(0,17):\n",
    "    stat = os.stat('%s/shard-%d/execution/stderr.submit'%(d,i))\n",
    "    print(time.ctime(stat.st_mtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/global/homes/b/bpb/bootcamp/bootcamp_jobs.sh','r') as bcf:\n",
    "    s = bcf.read()\n",
    "job_list = s.split('/global/common/software/m2650/mzmine_parameters/MZmine/MZmine-2.39/startMZmine_NERSC_Headless_Cori.sh ')\n",
    "job_list = job_list[1:]\n",
    "job_list = [j.strip() for j in job_list]\n",
    "n = 6000\n",
    " \n",
    "# using list comprehension \n",
    "final = [job_list[i * n:(i + 1) * n] for i in range((len(job_list) + n - 1) // n )]  \n",
    "# print (final) \n",
    "for i,small_list in enumerate(final):\n",
    "    d = {'jgi_mzmine.inputs':small_list}\n",
    "    with open('/global/homes/b/bpb/adap_job_array_%d.json'%i,'w') as fid:\n",
    "        json.dump(d,fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('/global/cscratch1/sd/bpb/mzmine_jobs/20190401_JGI_Metabolomics_Bootcamp_HILIC_D34FE46C/*.csv')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_batch_files = glob.glob('/project/projectdirs/metatlas/projects/jgi_projects/20190401_JGI_Metabolomics_Bootcamp_HILIC_D34FE46C/*.csv')\n",
    "all_batch_files  = all_batch_files + glob.glob('/project/projectdirs/metatlas/projects/jgi_projects/20190401_JGI_Metabolomics_Bootcamp_ReversePhase_18DCF58D/*.csv')\n",
    "all_batch_files = sorted(all_batch_files)\n",
    "all_batch_files = [os.path.basename(a).replace('_peak_height.csv','.xml') for a in all_batch_files]\n",
    "print('%s\\n%s'%(all_jobs[0].split(' ')[-1],all_batch_files[0]))\n",
    "os.path.basename(all_jobs[0].split(' ')[-1]) in all_batch_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_completed_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_completed_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break the jobs into 10 smaller job sets\n",
    "\n",
    "job_template = \"\"\"#!/bin/bash\n",
    "#SBATCH --qos=regular\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --constraint=haswell\n",
    "#SBATCH --error=\"gp.err\"\n",
    "#SBATCH --output=\"gp.out\"\n",
    "export MALLOC_ARENA_MAX=1\n",
    "\n",
    "source jobs.sh\n",
    "\"\"\"\n",
    "\n",
    "# job_template = \"\"\"#!/bin/bash\n",
    "# #SBATCH -t 00:15:00\n",
    "# #SBATCH -C haswell\n",
    "# #SBATCH -N 1\n",
    "# #SBATCH -q realtime\n",
    "# #SBATCH -A m1541\n",
    "# #SBATCH --exclusive\n",
    "\n",
    "# source jobs.sh\n",
    "# \"\"\"\n",
    "\n",
    "# job_template=\"\"\"#!/bin/bash\n",
    "# #SBATCH -N 4\n",
    "# #SBATCH -C haswell\n",
    "# #SBATCH --exclusive\n",
    "# #SBATCH -q realtime\n",
    "# #SBATCH --error=\"gp.err\"\n",
    "# #SBATCH --output=\"gp.out\"\n",
    "# #SBATCH -A m1541\n",
    "# #SBATCH -t 04:00:00\n",
    "# #SBATCH -L project\n",
    "\n",
    "# # cori specific tells it to not allocate memory on a per thread basis Java\n",
    "# export MALLOC_ARENA_MAX=1\n",
    "\n",
    "# module load taskfarmer\n",
    "# export THREADS=4\n",
    "# runcommands.sh jobs.sh\n",
    "# \"\"\"\n",
    "\n",
    "# job_template = \"\"\"#!/bin/bash\n",
    "# #SBATCH -N 20 -c 64\n",
    "# #SBATCH --error=\"gp.err\"\n",
    "# #SBATCH -L project \n",
    "# #SBATCH --output=\"gp.out\"\n",
    "# #SBATCH --qos=premium\n",
    "# #SBATCH -A m1541\n",
    "# #SBATCH --constraint=haswell\n",
    "# #SBATCH -L project\n",
    "# #SBATCH -t 24:00:00\n",
    "\n",
    "\n",
    "# export HDF5_USE_FILE_LOCKING=FALSE\n",
    "\n",
    "# module load taskfarmer\n",
    "# export THREADS=4\n",
    "# runcommands.sh jobs.sh\"\"\"\n",
    "\n",
    "with open('/global/homes/b/bpb/bootcamp/bootcamp_jobs.sh','r') as bcf:\n",
    "    s = bcf.read()\n",
    "\n",
    "# remove jobs that have been done\n",
    "all_jobs = s.split('\\n')\n",
    "\n",
    "all_completed_jobs = glob.glob('/project/projectdirs/metatlas/projects/jgi_projects/20190401_JGI_Metabolomics_Bootcamp_HILIC_D34FE46C/*.csv')\n",
    "all_completed_jobs  = all_completed_jobs + glob.glob('/project/projectdirs/metatlas/projects/jgi_projects/20190401_JGI_Metabolomics_Bootcamp_ReversePhase_18DCF58D/*.csv')\n",
    "all_completed_jobs = sorted(all_completed_jobs)\n",
    "all_completed_jobs = [os.path.basename(a).replace('_peak_height.csv','.xml') for a in all_completed_jobs]\n",
    "\n",
    "filtered_jobs = [x for x in all_jobs if os.path.basename(x.split(' ')[-1]) not in all_completed_jobs]\n",
    "len(all_jobs),len(filtered_jobs),len(all_completed_jobs)\n",
    "\n",
    "# filtered_jobs = filtered_jobs[:500]\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in xrange(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "        \n",
    "chunked_jobs = list(chunks(filtered_jobs,120))\n",
    "print('rm -r /global/homes/b/bpb/bootcamp/jobs/job_chunk_*')\n",
    "with open('/global/homes/b/bpb/bootcamp/knl_job_file.sh','w') as knl_file:\n",
    "    for i,cj in enumerate(chunked_jobs):\n",
    "        d = '/global/homes/b/bpb/bootcamp/jobs/job_chunk_%d'%i\n",
    "        if not os.path.isdir(d):\n",
    "            os.mkdir(d)\n",
    "        job_file = os.path.join(d,'jobs_%d.sh'%i)\n",
    "        sbatch_file = os.path.join(d,'job-%d.sbatch'%i)\n",
    "    #     job_string = '\\n'.join(cj)\n",
    "        small_chunks = list(chunks(cj,4)) #make make job chunks in 3 each\n",
    "        all_small_jobs = []\n",
    "        for sc in small_chunks:\n",
    "            all_small_jobs.append(' & \\n'.join(sc))\n",
    "        job_string = ' &\\nwait\\n'.join(all_small_jobs)\n",
    "        job_string = '%s &\\nwait\\n'%job_string\n",
    "        with open(job_file,'w') as fid: # write all the jobs to the file\n",
    "            fid.write('%s'%job_string)\n",
    "        with open(sbatch_file,'w') as fid:\n",
    "            fid.write('%s'%job_template.replace('jobs.sh',job_file).replace('gp.','log_%d_job.'%i))\n",
    "        knl_file.write('sbatch %s\\n'%sbatch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "mass spec cori",
   "language": "python",
   "name": "nersc_python27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
